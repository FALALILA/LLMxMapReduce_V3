#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Search Host Layer

è¿™æ˜¯MCPæ¶æ„ä¸­çš„Hostå±‚ï¼Œè´Ÿè´£ï¼š
1. æä¾›é«˜çº§ä¸šåŠ¡æ¥å£
2. åè°ƒæœç´¢æµç¨‹
3. è°ƒç”¨Clientå±‚ä¸MCPæœåŠ¡å™¨é€šä¿¡
4. ä¸æš´éœ²åº•å±‚MCPå·¥å…·å®ç°ç»†èŠ‚
"""

import asyncio
import json
import logging
import os
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

from .llm_search_mcp_client import create_mcp_client

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LLMSearchHost:
    """
    LLMæœç´¢å®¿ä¸»å±‚
    
    æä¾›é«˜çº§ä¸šåŠ¡æ¥å£ï¼Œéšè—MCPå®ç°ç»†èŠ‚
    """
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path
        self.mcp_client = None
        self._connected = False
        self.env_config = self._load_environment_config()
        
    def _load_environment_config(self):
        """åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶"""
        try:
            config_paths = [
                "new/config/environment_config.json",
                "config/environment_config.json",
                os.path.join(os.path.dirname(__file__), "..", "..", "config", "environment_config.json")
            ]

            for config_path in config_paths:
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        return json.load(f)

            logger.warning("Environment config file not found, using defaults")
            return {
                "models": {"default_model": "gemini-2.5-flash", "default_infer_type": "OpenAI"},
                "search_settings": {"default_query_count": 30, "default_total_urls": 200, "default_top_n": 70}
            }
        except Exception as e:
            logger.error(f"Failed to load environment config: {e}")
            return {}
    
    async def connect(self):
        """è¿æ¥åˆ°MCPæœåŠ¡å™¨"""
        if self._connected:
            return
            
        try:
            # å‡†å¤‡MCPæœåŠ¡å™¨é…ç½®
            server_config = self._prepare_server_config()
            
            # åˆ›å»ºMCPå®¢æˆ·ç«¯
            self.mcp_client = await create_mcp_client(server_config)
            self._connected = True
            
            logger.info("Successfully connected to LLM Search MCP Server")
            
        except Exception as e:
            logger.error(f"Failed to connect to MCP server: {e}")
            raise
    
    def _prepare_server_config(self) -> Dict[str, Any]:
        """å‡†å¤‡MCPæœåŠ¡å™¨é…ç½®"""
        # åªè®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼Œé¿å…ç³»ç»Ÿç¯å¢ƒå˜é‡å†²çª
        # ä½†ä¿ç•™ä¸€äº›å…³é”®çš„ç³»ç»Ÿç¯å¢ƒå˜é‡ä»¥ç¡®ä¿Pythonæ­£å¸¸è¿è¡Œ
        env_vars = {
            "PYTHONPATH": ".",
            "PATH": os.environ.get("PATH", ""),
            "SYSTEMROOT": os.environ.get("SYSTEMROOT", ""),
            "TEMP": os.environ.get("TEMP", ""),
            "TMP": os.environ.get("TMP", ""),
        }

        # ä»é…ç½®æ–‡ä»¶è®¾ç½®APIå¯†é’¥
        api_keys = self.env_config.get("api_keys", {})

        openai_config = api_keys.get("openai", {})
        if openai_config.get("api_key"):
            env_vars["OPENAI_API_KEY"] = openai_config["api_key"]
            logger.info("âœ… OPENAI_API_KEY å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨")
        if openai_config.get("base_url"):
            env_vars["OPENAI_BASE_URL"] = openai_config["base_url"]
            env_vars["OPENAI_API_BASE"] = openai_config["base_url"]  # å…¼å®¹æ€§
            logger.info("âœ… OPENAI_BASE_URL å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨")

        search_engines = api_keys.get("search_engines", {})
        if search_engines.get("serpapi_key"):
            env_vars["SERPAPI_KEY"] = search_engines["serpapi_key"]
            env_vars["SERP_API_KEY"] = search_engines["serpapi_key"]  # å…¼å®¹æ€§
            logger.info(f"âœ… SERPAPI_KEY å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨: {search_engines['serpapi_key'][:10]}...")
        if search_engines.get("bing_subscription_key"):
            env_vars["BING_SEARCH_V7_SUBSCRIPTION_KEY"] = search_engines["bing_subscription_key"]
            logger.info("âœ… BING_SEARCH_V7_SUBSCRIPTION_KEY å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨")

        # æ·»åŠ Google API Keyï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if openai_config.get("api_key"):
            env_vars["GOOGLE_API_KEY"] = openai_config["api_key"]

        # éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
        logger.info(f"ğŸ” ç¯å¢ƒå˜é‡éªŒè¯:")
        logger.info(f"  - SERPAPI_KEY: {'å·²è®¾ç½®' if env_vars.get('SERPAPI_KEY') else 'æœªè®¾ç½®'}")
        logger.info(f"  - OPENAI_API_KEY: {'å·²è®¾ç½®' if env_vars.get('OPENAI_API_KEY') else 'æœªè®¾ç½®'}")
        logger.info(f"  - ç¯å¢ƒå˜é‡æ€»æ•°: {len(env_vars)}")

        # è·å–é¡¹ç›®æ ¹ç›®å½•
        current_file = Path(__file__).resolve()
        project_root = current_file.parent.parent.parent

        # ä½¿ç”¨æ¨¡å—å¯¼å…¥æ–¹å¼å¯åŠ¨ï¼Œä¸é…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
        return {
            "command": "python",
            "args": ["-m", "src.search.llm_search_mcp_server"],
            "env": env_vars,
            "cwd": str(project_root)
        }
    
    async def disconnect(self):
        """æ–­å¼€MCPè¿æ¥"""
        if self.mcp_client:
            await self.mcp_client.disconnect()
            self.mcp_client = None
        self._connected = False
        logger.info("Disconnected from MCP server")
    
    async def search_literature(self, topic: str, description: str = "", top_n: int = 20) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ–‡çŒ®æœç´¢æµç¨‹
        
        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            description: ä¸»é¢˜æè¿°
            top_n: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ–‡çŒ®æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._connected:
            await self.connect()
            
        try:
            logger.info(f"Starting literature search for topic: '{topic}'")
            
            # ä»é…ç½®è·å–é»˜è®¤å‚æ•°
            search_settings = self.env_config.get("search_settings", {})
            query_count = search_settings.get("default_query_count", 30)
            
            # æ­¥éª¤1: ç”Ÿæˆæœç´¢æŸ¥è¯¢
            logger.info("Step 1: Generating search queries")
            query_result = await self.mcp_client.call_tool(
                "generate_search_queries",
                {
                    "topic": topic,
                    "description": description
                }
            )
            
            if not query_result or "query_file_path" not in query_result:
                raise ValueError("Failed to generate search queries")

            query_file_path = query_result["query_file_path"]
            query_count = query_result.get("query_count", 0)
            logger.info(f"Generated {query_count} search queries, saved to: {query_file_path}")

            # æ­¥éª¤2: æ‰§è¡Œç½‘ç»œæœç´¢
            logger.info("Step 2: Performing web search")
            search_result = await self.mcp_client.call_tool(
                "web_search",
                {
                    "query_file_path": query_file_path,  # ä¼ é€’æ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯æŸ¥è¯¢åˆ—è¡¨
                    "topic": topic,
                    "top_n": search_settings.get("default_total_urls", 200)
                }
            )
            
            if not search_result or "urls" not in search_result:
                raise ValueError("Failed to perform web search")
                
            urls = search_result["urls"]
            logger.info(f"Found {len(urls)} URLs from web search")
            
            # æ­¥éª¤3: çˆ¬å–å’Œåˆ†æå†…å®¹
            logger.info("Step 3: Crawling and analyzing content")
            # è·å–URLæ–‡ä»¶è·¯å¾„ï¼ˆä»web_searchç»“æœä¸­ï¼‰
            url_file_path = search_result.get("url_file_path")
            crawl_result = await self.mcp_client.call_tool(
                "crawl_urls",
                {
                    "topic": topic,
                    "url_file_path": url_file_path,  # ä¼ é€’æ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯URLåˆ—è¡¨
                    "top_n": top_n,
                    "similarity_threshold": search_settings.get("default_similarity_threshold", 30)
                }
            )
            
            if not crawl_result:
                raise ValueError("Failed to crawl URLs")
            
            # æå–æœ€ç»ˆç»“æœ
            final_results = crawl_result.get("final_results", [])
            logger.info(f"Successfully retrieved {len(final_results)} literature papers")
            
            # ä¿å­˜ç»“æœ
            self._save_results(final_results, topic)
            
            return final_results
            
        except Exception as e:
            logger.error(f"Error in literature search: {e}")
            raise
    
    def _save_results(self, results: List[Dict[str, Any]], topic: str) -> str:
        """ä¿å­˜æœç´¢ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = Path(__file__).parent.parent.parent / "test"
            save_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_topic = safe_topic.replace(' ', '_')[:50]
            filename = f"literature_results_{safe_topic}_{timestamp}.json"
            filepath = save_dir / filename
            
            # ä¿å­˜æ•°æ®
            save_data = {
                "topic": topic,
                "timestamp": timestamp,
                "total_results": len(results),
                "results": results,
                "metadata": {
                    "generated_by": "llm_search_host.py",
                    "version": "1.0",
                    "description": f"Literature search results for topic: {topic}"
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Results saved to: {filepath}")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return ""
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.disconnect()


# ä¾¿æ·å‡½æ•°
async def search_literature(topic: str, description: str = "", top_n: int = 20) -> List[Dict[str, Any]]:
    """
    ä¾¿æ·çš„æ–‡çŒ®æœç´¢å‡½æ•°
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        description: ä¸»é¢˜æè¿°
        top_n: è¿”å›ç»“æœæ•°é‡
        
    Returns:
        æ–‡çŒ®æœç´¢ç»“æœåˆ—è¡¨
    """
    async with LLMSearchHost() as host:
        return await host.search_literature(topic, description, top_n)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python llm_search_host.py <topic> [description] [top_n]")
        sys.exit(1)
    
    topic = sys.argv[1]
    description = sys.argv[2] if len(sys.argv) > 2 else ""
    top_n = int(sys.argv[3]) if len(sys.argv) > 3 else 20
    
    print(f"ğŸ” Starting literature search for: {topic}")
    if description:
        print(f"ğŸ“ Description: {description}")
    print(f"ğŸ¯ Target papers: {top_n}")
    print("-" * 50)
    
    try:
        results = asyncio.run(search_literature(topic, description, top_n))
        print(f"\nâœ… Search completed! Found {len(results)} papers")
    except Exception as e:
        print(f"\nâŒ Search failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
