#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analyse Interface with LLM Host

ä½¿ç”¨LLMHostè¿›è¡Œæ™ºèƒ½ä»»åŠ¡å¤„ç†çš„åˆ†ææ¥å£
"""

import os
import json
import logging
import asyncio
import time
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from request.wrapper import RequestWrapper
from .llm_host import LLMHost
    
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AnalyseLLMHostInterface:
    """
    ä½¿ç”¨LLMHostçš„åˆ†ææ¥å£
    
    æä¾›æ™ºèƒ½ä»»åŠ¡å¤„ç†èƒ½åŠ›ï¼Œè®©LLMè‡ªä¸»é€‰æ‹©å’Œè°ƒç”¨å·¥å…·
    """
    
    def __init__(self,
                 base_dir: str = "new/test",
                 config_path: Optional[str] = None):

        self.base_dir = Path(base_dir)
        self.config_path = config_path
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.config_dir = self.base_dir / "config"
        self.config_dir.mkdir(exist_ok=True)

        # é¦–å…ˆåŠ è½½ç¯å¢ƒé…ç½®
        self._load_environment_config()

        # ç¡®ä¿env_configå·²è®¾ç½®
        # if not hasattr(self, 'env_config') or self.env_config is None:
        #     logger.error("env_config not properly loaded, using defaults")
        #     self.env_config = {
        #         "models": {"default_model": "gemini-2.5-flash", "default_infer_type": "OpenAI"},
        #         "analyse_settings": {"max_interaction_rounds": 3, "max_context_messages": 10}
        #     }

        # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼Œç¡®ä¿æœ‰é»˜è®¤å€¼
        try:
            self.max_interaction_rounds = self.env_config.get("analyse_settings", {}).get("max_interaction_rounds", 3)
            self.llm_model = self.env_config.get("models", {}).get("default_model", "gemini-2.5-flash")
            self.llm_infer_type = self.env_config.get("models", {}).get("default_infer_type", "OpenAI")
        except Exception as e:
            logger.error(f"Failed to load environment config: {e}")
            # è®¾ç½®é»˜è®¤å€¼
            self.max_interaction_rounds = 3
            self.llm_model = "gemini-2.5-flash"
            self.llm_infer_type = "OpenAI"

        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        self.conversation_history = []  # å­˜å‚¨å®Œæ•´çš„å¯¹è¯å†å²

        # åˆå§‹åŒ–LLMå®¿ä¸»
        self.llm_host = LLMHost()

        # åˆå§‹åŒ–loggerï¼ˆä¿®å¤Windowsç¼–ç é—®é¢˜ï¼‰
        self.logger = logging.getLogger(__name__)

        # ç¡®ä¿loggerä½¿ç”¨UTF-8ç¼–ç ï¼Œé¿å…Windows GBKç¼–ç é—®é¢˜
        for handler in logging.root.handlers:
            if hasattr(handler, 'stream') and hasattr(handler.stream, 'reconfigure'):
                try:
                    handler.stream.reconfigure(encoding='utf-8')
                except:
                    pass

# MemoryåŠŸèƒ½å·²ç§»é™¤

        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆåœ¨åŠ è½½é…ç½®ä¹‹åï¼‰
        self._init_llm_components()

        self._load_config()

# _clear_memory_on_initæ–¹æ³•å·²ç§»é™¤

    async def cleanup(self):
        """
        æ¸…ç†èµ„æºï¼Œå…³é—­è¿æ¥
        """
        try:
            if hasattr(self, 'llm_host') and self.llm_host:
                try:
                    await self.llm_host.disconnect()
                except Exception as disconnect_error:
                    # Log the error but don't let it stop the cleanup process
                    self.logger.warning(f"LLM host disconnect had issues (continuing cleanup): {disconnect_error}")
                finally:
                    # Always clear the host reference
                    self.llm_host = None
            self.logger.info("èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
            # Force cleanup
            if hasattr(self, 'llm_host'):
                self.llm_host = None

    def _load_environment_config(self):
        """åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„ç»Ÿä¸€é…ç½®æ–‡ä»¶è·¯å¾„
            config_paths = [
                "new/config/unified_config.json",
                "config/unified_config.json",
                os.path.join(os.path.dirname(__file__), "..", "..", "config", "unified_config.json")
            ]

            for config_path in config_paths:
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        self.env_config = json.load(f)
                    logger.info(f"Environment config loaded from: {config_path}")

                    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¡®ä¿API keyæ­£ç¡®ä¼ é€’
                    self._set_environment_variables()
                    return

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            logger.warning("Environment config file not found, using defaults")
            self.env_config = {
                "models": {
                    "default_model": "gemini-2.0-flash",
                    "default_infer_type": "OpenAI"
                },
                "analyse_settings": {
                    "max_interaction_rounds": 3,
                    "max_context_messages": 10,
                    "web_search_timeout": 30000.0,
                    "crawl_urls_timeout": 30000.0,
                    "mcp_tool_timeout": 30000.0
                }
            }
        except Exception as e:
            logger.error(f"Failed to load environment config: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
            self.env_config = {
                "models": {
                    "default_model": "gemini-2.5-flash",
                    "default_infer_type": "OpenAI"
                },
                "analyse_settings": {
                    "max_interaction_rounds": 3,
                    "max_context_messages": 10,
                    "web_search_timeout": 30000.0,
                    "crawl_urls_timeout": 30000.0,
                    "mcp_tool_timeout": 30000.0
                }
            }

    def _set_environment_variables(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿API keyæ­£ç¡®ä¼ é€’"""
        try:
            api_keys = self.env_config.get("api_keys", {})

            # è®¾ç½®OpenAI APIé…ç½®
            openai_config = api_keys.get("openai", {})
            if openai_config.get("api_key"):
                os.environ["OPENAI_API_KEY"] = openai_config["api_key"]
                logger.info("âœ… OPENAI_API_KEY å·²è®¾ç½®")
            if openai_config.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = openai_config["base_url"]
                logger.info("âœ… OPENAI_BASE_URL å·²è®¾ç½®")

            # è®¾ç½®æœç´¢å¼•æ“APIå¯†é’¥
            search_engines = api_keys.get("search_engines", {})
            if search_engines.get("serpapi_key"):
                os.environ["SERPAPI_KEY"] = search_engines["serpapi_key"]
                logger.info(f"âœ… SERPAPI_KEY å·²è®¾ç½®: {search_engines['serpapi_key'][:10]}...")

            if search_engines.get("bing_subscription_key"):
                os.environ["BING_SEARCH_V7_SUBSCRIPTION_KEY"] = search_engines["bing_subscription_key"]
                logger.info("âœ… BING_SEARCH_V7_SUBSCRIPTION_KEY å·²è®¾ç½®")

        except Exception as e:
            logger.error(f"Failed to set environment variables: {e}")

        logger.info(f"AnalyseLLMHostInterface initialized:")
        logger.info(f"  - Base directory: {self.base_dir}")
        logger.info(f"  - Max interaction rounds: {self.max_interaction_rounds}")
        logger.info(f"  - LLM model: {self.llm_model}")
        logger.info(f"  - Using LLMHost for intelligent task processing")

    def _init_llm_components(self):
        try:
            self.llm_wrapper = RequestWrapper(
                model=self.llm_model,
                infer_type=self.llm_infer_type
            )
            logger.info("LLM wrapper initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize LLM wrapper: {e}")
            raise

    async def analyse(self, topic: str, description: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ™ºèƒ½ä»»åŠ¡åˆ†æ - ä½¿ç”¨LLMHostè¿›è¡Œæ™ºèƒ½å·¥å…·é€‰æ‹©
        
        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            description: ä¸»é¢˜æè¿°
            
        Returns:
            åˆ†æç»“æœ
        """
        logger.info(f"Starting intelligent task analysis for topic: '{topic}'")

        # è®°å¿†åˆå§‹åŒ–
        self.conversation_history.clear()
        logger.info("=== è®°å¿†åˆå§‹åŒ–å®Œæˆ ===")

        try:
            # ç¬¬ä¸€è½®äº¤äº’ï¼šä¸»é¢˜æ‰©å†™
            logger.info("=== ç¬¬ä¸€è½®äº¤äº’ï¼šä¸»é¢˜æ‰©å†™ ===")
            user_msg_1 = f"è¯·æ‰©å†™ä¸»é¢˜ï¼š{topic}ã€‚åŸå§‹æè¿°ï¼š{description or 'æ— '}"
            expanded_topic = await self._llm_interaction_round_1(user_msg_1)
            logger.info(f"ä¸»é¢˜æ‰©å†™å®Œæˆ: {expanded_topic[:100]}...")

            # ç¬¬äºŒè½®ï¼šä½¿ç”¨LLMHostè¿›è¡Œæ™ºèƒ½ä»»åŠ¡å¤„ç†
            logger.info("=== ç¬¬äºŒè½®ï¼šæ™ºèƒ½ä»»åŠ¡å¤„ç† ===")
            
            # æ„å»ºä»»åŠ¡æè¿°
            task_description = f"æ‰§è¡Œæ–‡çŒ®æœç´¢ä»»åŠ¡ï¼š{topic}"
            context = f"æ‰©å†™åçš„ä¸»é¢˜æè¿°ï¼š{expanded_topic}"

            # ä½¿ç”¨LLMHostæ‰§è¡Œæ™ºèƒ½ä»»åŠ¡å¤„ç†
            result = await self.llm_host.process_task(task_description, context)

            logger.info(f"âœ… Intelligent analysis completed successfully")
            logger.info(f"Status: {result.get('status', 'unknown')}")
            logger.info(f"Rounds used: {result.get('rounds_used', 0)}")

            return result

        except Exception as e:
            logger.error(f"Error in intelligent task analysis: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise

    async def _llm_interaction_round_1(self, user_message: str) -> str:
        """ç¬¬ä¸€è½®äº¤äº’ï¼šä¸»é¢˜æ‰©å†™"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€ä¸ªç ”ç©¶ä¸»é¢˜ï¼Œè¯·ä½ æ‰©å†™è¿™ä¸ªä¸»é¢˜ï¼Œæä¾›è¯¦ç»†çš„ç ”ç©¶æè¿°ã€‚

è¯·ä»å¤šä¸ªè§’åº¦è¿›è¡Œåˆ†æï¼Œç”Ÿæˆä¸€æ®µä¸“ä¸šä¸”å…¨é¢çš„æè¿°ï¼Œè¿™ä¸ªæè¿°å°†ç”¨äºåç»­çš„æ–‡çŒ®æœç´¢ã€‚

åªè¿”å›æ‰©å†™åçš„ä¸»é¢˜æè¿°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt}
        ] + self.conversation_history

        # è°ƒç”¨LLM
        response = await self.llm_wrapper.async_request(messages)

        # æ·»åŠ assistantå“åº”åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            "role": "assistant",
            "content": response
        })

        return response

    def _load_config(self):
        try:
            if self.config_path and os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            else:
                self.config = {
                    "search": {
                        "default_top_n": 20,
                        "default_engine": "google",
                        "similarity_threshold": 80
                    },
                    "interaction": {
                        "timeout_seconds": 0,  # æ— è¶…æ—¶é™åˆ¶
                        "auto_continue": False
                    }
                }
            logger.info("Configuration loaded successfully")
        except Exception as e:
            logger.warning(f"Failed to load config, using defaults: {e}")
            self.config = {}


# ä¾¿æ·å‡½æ•°
async def analyse_with_llm_host(task: str, description: Optional[str] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„æ™ºèƒ½åˆ†æå‡½æ•°
    
    Args:
        task: ç ”ç©¶ä¸»é¢˜
        description: ä¸»é¢˜æè¿°
        
    Returns:
        åˆ†æç»“æœ
    """
    analyser = AnalyseLLMHostInterface()
    try:
        return await analyser.analyse(task, description)
    finally:
        await analyser.cleanup()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python analyse_llm_host.py <topic> [description]")
        sys.exit(1)

    topic = sys.argv[1]
    description = sys.argv[2] if len(sys.argv) > 2 else None

    print(f"ğŸ¤– å¼€å§‹æ™ºèƒ½åˆ†æä¸»é¢˜: {topic}")
    if description:
        print(f"ğŸ“ æè¿°: {description}")
    print("-" * 50)

    try:
        import asyncio
        analysis_result = asyncio.run(analyse_with_llm_host(topic, description))
        print(f"\nâœ… æ™ºèƒ½åˆ†æå®Œæˆï¼")
        print(f"çŠ¶æ€: {analysis_result.get('status', 'unknown')}")
        print(f"ä½¿ç”¨è½®æ•°: {analysis_result.get('rounds_used', 0)}")
        print(f"ç»“æœ: {analysis_result.get('result', 'No result')}")
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
