#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import asyncio
import json
import logging
import sys
import os
import time
import re
import traceback
from typing import Dict, Any, List

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
try:
    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':
        sys.stdout.reconfigure(encoding='utf-8')
    if hasattr(sys.stderr, 'reconfigure') and sys.stderr.encoding != 'utf-8':
        sys.stderr.reconfigure(encoding='utf-8')
except (AttributeError, OSError):
    # åœ¨æŸäº›ç¯å¢ƒä¸­å¯èƒ½æ— æ³•é‡æ–°é…ç½®ç¼–ç ï¼Œå¿½ç•¥é”™è¯¯
    pass

from mcp.server import Server
from mcp.types import Resource, Tool, TextContent
import mcp.server.stdio

try:
    from crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    AsyncWebCrawler = None
    CacheMode = None
    CrawlerRunConfig = None

# ä¼˜å…ˆä»å½“å‰é¡¹ç›®å¯¼å…¥LLM_search
from .LLM_search import LLM_search
# print(f"âœ… LLM_search ä»ç›¸å¯¹å¯¼å…¥æˆåŠŸ")

# å¯¼å…¥HTMLæ¸…ç†æ¨¡å—
try:
    from .clean.html_extrator import CommonCrawlWARCExtractor, JusTextExtractor, ResiliparseExtractor
    HTML_CLEANER_AVAILABLE = True
except ImportError as e:
    HTML_CLEANER_AVAILABLE = False
    CommonCrawlWARCExtractor = None
    JusTextExtractor = None
    ResiliparseExtractor = None

from src.exceptions import AnalyseError
from request import RequestWrapper

import logging.handlers

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'logs')
os.makedirs(log_dir, exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'mcp_server.log'), encoding='utf-8'),
        logging.StreamHandler(sys.stderr)  # è¾“å‡ºåˆ°stderrä»¥ä¾¿å®¢æˆ·ç«¯èƒ½çœ‹åˆ°
    ]
)

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–HTMLæ¸…ç†å™¨
_html_extractor = None
if HTML_CLEANER_AVAILABLE:
    try:
        # ä½¿ç”¨JusTextç®—æ³•ä½œä¸ºé»˜è®¤æ¸…ç†å™¨
        _html_extractor = CommonCrawlWARCExtractor(algorithm=JusTextExtractor())
        logger.info("âœ… HTML cleaner initialized with JusText algorithm")
    except Exception as e:
        logger.warning(f"âŒ Failed to initialize HTML cleaner: {e}")
        logger.info("Will fall back to basic HTML cleaning using regex")
        # ä¸è®¾ç½®HTML_CLEANER_AVAILABLEä¸ºFalseï¼Œè®©åŸºæœ¬æ¸…ç†åŠŸèƒ½ç»§ç»­å·¥ä½œ
        _html_extractor = None
else:
    logger.warning("âŒ HTML cleaner not available, will use basic text extraction")

app = Server("llm-search-server")

def load_server_config():
    """ä»ç»Ÿä¸€é…ç½®æ–‡ä»¶åŠ è½½Serveré…ç½®"""
    # åŠ è½½ç»Ÿä¸€é…ç½®
    config_path = os.path.join(os.path.dirname(__file__), '..', '..', 'config', 'unified_config.json')

    try:
        # è¯»å–ç»Ÿä¸€é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # è®¾ç½®ç¯å¢ƒå˜é‡
        api_keys = config.get("api_keys", {})

        # è®¾ç½®OpenAI APIé…ç½®
        openai_config = api_keys.get("openai", {})
        if openai_config.get("api_key"):
            os.environ["OPENAI_API_KEY"] = openai_config["api_key"]
        if openai_config.get("base_url"):
            os.environ["OPENAI_BASE_URL"] = openai_config["base_url"]

        # è®¾ç½®æœç´¢å¼•æ“APIå¯†é’¥ï¼ˆä¼˜å…ˆä½¿ç”¨å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰
        search_engines = api_keys.get("search_engines", {})

        # SERPAPIå¯†é’¥è®¾ç½®
        if not os.environ.get("SERPAPI_KEY") and search_engines.get("serpapi_key"):
            os.environ["SERPAPI_KEY"] = search_engines["serpapi_key"]
            logger.info(f"âœ… SERPAPI_KEY ä»é…ç½®æ–‡ä»¶è®¾ç½®: {search_engines['serpapi_key'][:10]}...")
        elif os.environ.get("SERPAPI_KEY"):
            logger.info(f"âœ… SERPAPI_KEY ä½¿ç”¨ç¯å¢ƒå˜é‡: {os.environ['SERPAPI_KEY'][:10]}...")
        else:
            logger.warning("âŒ SERPAPI_KEY æœªåœ¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°")

        # Bingå¯†é’¥è®¾ç½®
        if not os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY") and search_engines.get("bing_subscription_key"):
            os.environ["BING_SEARCH_V7_SUBSCRIPTION_KEY"] = search_engines["bing_subscription_key"]
            logger.info(f"âœ… BING_SEARCH_V7_SUBSCRIPTION_KEY ä»é…ç½®æ–‡ä»¶è®¾ç½®")
        elif os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY"):
            logger.info(f"âœ… BING_SEARCH_V7_SUBSCRIPTION_KEY ä½¿ç”¨ç¯å¢ƒå˜é‡")
        else:
            logger.warning("âŒ BING_SEARCH_V7_SUBSCRIPTION_KEY æœªåœ¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°")

        # éªŒè¯ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®
        serpapi_key = os.environ.get("SERPAPI_KEY")
        bing_key = os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY")
        logger.info(f"ğŸ” ç¯å¢ƒå˜é‡éªŒè¯ - SERPAPI_KEY: {'âœ… å·²è®¾ç½®' if serpapi_key else 'âŒ æœªè®¾ç½®'}")
        logger.info(f"ğŸ” ç¯å¢ƒå˜é‡éªŒè¯ - BING_KEY: {'âœ… å·²è®¾ç½®' if bing_key else 'âŒ æœªè®¾ç½®'}")

        # æ„å»ºæœåŠ¡å™¨é…ç½®
        models = config.get("models", {})
        search_settings = config.get("search_settings", {})
        timeout_settings = config.get("timeout_settings", {})
        crawling_settings = config.get("crawling_settings", {})
        mcp_settings = config.get("mcp_settings", {})
        prompts = config.get("prompts", {})

        # ç¡®ä¿ä½¿ç”¨æ›´æ–°åçš„æ¨¡å‹é…ç½®
        logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶è¯»å–çš„æ¨¡å‹è®¾ç½®: {models.get('default_model', 'N/A')}")

        # éªŒè¯å¿…éœ€çš„æ¨¡å‹é…ç½®
        required_models = ["default_model", "default_infer_type", "content_analysis_model",
                          "similarity_model", "page_refine_model"]
        for model_key in required_models:
            if not models.get(model_key):
                raise ValueError(f"Missing required model configuration: {model_key}")

        server_config = {
            # æ¨¡å‹é…ç½® - ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œä¸ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼
            "default_model": models.get("default_model"),
            "default_infer_type": models.get("default_infer_type"),
            "content_analysis_model": models.get("content_analysis_model"),
            "similarity_model": models.get("similarity_model"),
            "page_refine_model": models.get("page_refine_model"),

            # æœç´¢é…ç½®
            "default_engine": search_settings.get("default_engine", "google"),
            "default_query_count": search_settings.get("default_query_count", 30),
            "default_each_query_result": search_settings.get("default_each_query_result", 10),
            "default_total_urls": search_settings.get("default_total_urls", 200),
            "default_top_n": search_settings.get("default_top_n", 70),
            "default_similarity_threshold": search_settings.get("default_similarity_threshold", 30),
            "default_min_length": search_settings.get("default_min_length", 100),
            "default_max_length": search_settings.get("default_max_length", 1000000),  # å¢åŠ åˆ°100ä¸‡å­—ç¬¦

            # è¶…æ—¶é…ç½®ï¼ˆå•ä½ï¼šç§’ï¼‰
            "llm_request_timeout": timeout_settings.get("llm_request_timeout", 30),
            "web_search_timeout": timeout_settings.get("web_search_timeout", 0),
            "crawling_timeout": timeout_settings.get("crawling_timeout", 0),
            "single_url_crawl_timeout": timeout_settings.get("single_url_crawl_timeout", 60),
            "content_analysis_timeout": timeout_settings.get("content_analysis_timeout", 30),
            "similarity_scoring_timeout": timeout_settings.get("similarity_scoring_timeout", 30),
            "abstract_generation_timeout": timeout_settings.get("abstract_generation_timeout", 30),
            "abstract_tasks_wait_timeout": timeout_settings.get("abstract_tasks_wait_timeout", 300),

            # çˆ¬è™«é…ç½®
            "max_concurrent_crawls": crawling_settings.get("max_concurrent_crawls", 10),
            "page_timeout": crawling_settings.get("page_timeout", 60),
            "retry_attempts": crawling_settings.get("retry_attempts", 3),
            "cache_mode": crawling_settings.get("cache_mode", "BYPASS"),

            # MCPè®¾ç½®
            "query_cache_dir": mcp_settings.get("query_cache_dir", "query_cache"),
            "url_cache_dir": mcp_settings.get("url_cache_dir", "url_cache"),

            # æç¤ºè¯é…ç½®
            "prompts": prompts
        }

        logger.info(f"Environment config loaded successfully")
        logger.info(f"Server config: {server_config}")
        return server_config

    except Exception as e:
        logger.error(f"Failed to load environment config: {e}")
        logger.error("Configuration file config/unified_config.json is required but not found or invalid")
        raise FileNotFoundError("Configuration file config/unified_config.json is required but not found or invalid")

SERVER_CONFIG = load_server_config()
llm_search_instances = {}

# è¾…åŠ©å‡½æ•°ï¼šç”¨äºç”Ÿæˆå…¼å®¹ LLMxMapReduce_V2 å’Œ Survey æ ¼å¼çš„ JSON
def proc_title_to_str(origin_title: str) -> str:
    """
    å°†æ ‡é¢˜è½¬æ¢ä¸ºbibkeyæ ¼å¼
    å¤åˆ¶è‡ª LLMxMapReduce_V2/src/utils/process_str.py
    """
    if not origin_title:
        return ""

    title = origin_title.lower().strip()
    title = title.replace("-", "_")
    title = re.sub(r'[^\w\s\_]', '', title)
    title = title.replace(" ", "_")
    title = re.sub(r'_{2,}', '_', title)
    return title

def estimate_tokens(text: str) -> int:
    """
    ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
    ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šå•è¯æ•° * 1.3
    """
    if not text:
        return 0
    words = text.split()
    return int(len(words) * 1.3)

def extract_abstract(text: str, max_length: int = 500) -> str:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‘˜è¦
    ç®€å•å®ç°ï¼šå–å‰max_lengthä¸ªå­—ç¬¦
    """
    if not text:
        return ""

    # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    cleaned_text = re.sub(r'\s+', ' ', text.strip())

    if len(cleaned_text) <= max_length:
        return cleaned_text

    # å°è¯•åœ¨å¥å·å¤„æˆªæ–­ï¼Œé¿å…æˆªæ–­å¥å­
    truncated = cleaned_text[:max_length]
    last_period = truncated.rfind('.')

    if last_period > max_length * 0.7:  # å¦‚æœå¥å·ä½ç½®åˆç†
        return truncated[:last_period + 1]
    else:
        return truncated + "..."
@app.list_resources()
async def list_resources() -> List[Resource]:
    return [
        Resource(
            uri="llm://search/prompts",
            name="LLM Search Prompts",
            description="LLMæœç´¢ç›¸å…³çš„æç¤ºè¯æ¨¡æ¿",
            mimeType="application/json"
        )
    ]

@app.read_resource()
async def read_resource(uri: str) -> List[TextContent]:
    if uri == "llm://search/prompts":
        # ä»é…ç½®æ–‡ä»¶è¯»å–æç¤ºè¯
        prompts = SERVER_CONFIG.get("prompts", {})
        return [TextContent(
            type="text",
            text=json.dumps(prompts, ensure_ascii=False, indent=2)
        )]
    else:
        raise ValueError(f"Unknown resource: {uri}")

@app.list_tools()
async def list_tools() -> List[Tool]:
    return [
        Tool(
            name="generate_search_queries",
            description="åŸºäºLLMç”Ÿæˆä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢ã€‚éœ€è¦æä¾›ç ”ç©¶ä¸»é¢˜ï¼Œè¿”å›ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡å’Œä¿å­˜æ–‡ä»¶è·¯å¾„ï¼Œä¸è¿”å›å…·ä½“æŸ¥è¯¢å†…å®¹ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": "ç ”ç©¶ä¸»é¢˜"
                    },
                    "description": {
                        "type": "string",
                        "description": "ä¸»é¢˜çš„å¯é€‰æè¿°æˆ–ä¸Šä¸‹æ–‡"
                    }
                },
                "required": ["topic"]
            }
        ),
        Tool(
            name="web_search",
            description="æ‰§è¡Œç½‘ç»œæœç´¢å¹¶æ”¶é›†URLã€‚éœ€è¦æä¾›ä¸»é¢˜ï¼Œè¿”å›æœç´¢åˆ°çš„URLæ•°é‡å’Œä¿å­˜æ–‡ä»¶è·¯å¾„ï¼Œä¸è¿”å›å…·ä½“URLåˆ—è¡¨ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": "ç”¨äºç›¸å…³æ€§è¿‡æ»¤çš„ä¸»è¦ä¸»é¢˜"
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "è¿”å›çš„æœ€ç›¸å…³URLæ•°é‡",
                        "default": 200
                    }
                },
                "required": ["topic"]
            }
        ),
        Tool(
            name="crawl_urls",
            description="çˆ¬å–URLå†…å®¹å¹¶è¿›è¡Œæ™ºèƒ½å¤„ç†ã€‚éœ€è¦æä¾›ç ”ç©¶ä¸»é¢˜ï¼Œè¿”å›çˆ¬å–çš„URLæˆåŠŸæ•°é‡ã€æœ€ç»ˆç»“æœæ•°é‡å’Œä¿å­˜æ–‡ä»¶è·¯å¾„ï¼Œä¸è¿”å›å…·ä½“æ–‡ç« å†…å®¹ã€‚",
            inputSchema={
                "type": "object",
                "properties": {
                    "topic": {
                        "type": "string",
                        "description": "ç ”ç©¶ä¸»é¢˜ï¼Œç”¨äºå†…å®¹è¿‡æ»¤å’Œç›¸ä¼¼åº¦è¯„åˆ†"
                    },
                    "top_n": {
                        "type": "integer",
                        "description": "è¿”å›çš„æœ€é«˜è´¨é‡ç»“æœæ•°é‡",
                        "default": 70
                    }
                },
                "required": ["topic"]
            }
        )
    ]
@app.call_tool()
async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
    """è°ƒç”¨å·¥å…·"""
    global llm_search_instances

    try:
        if name == "generate_search_queries":
            result = await _generate_search_queries(
                arguments["topic"],
                arguments.get("description", ""),
                arguments.get("model")  
            )
        elif name == "web_search":
            result = await _web_search(
                arguments.get("query_file_path"),  # æ”¹ä¸ºä»å‚æ•°è·å–æ–‡ä»¶è·¯å¾„
                arguments["topic"],
                arguments.get("top_n"),
                arguments.get("engine")
            )

        elif name == "crawl_urls":
            result = await _crawl_urls(
                arguments["topic"],
                arguments.get("url_file_path"),  # æ”¹ä¸ºä»å‚æ•°è·å–æ–‡ä»¶è·¯å¾„
                arguments.get("top_n"),
                arguments.get("model"),
                arguments.get("similarity_threshold"),
                arguments.get("min_length"),
                arguments.get("max_length")
            )
        else:
            raise ValueError(f"Unknown tool: {name}")

        return [TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]

    except Exception as e:
        logger.error(f"Error calling tool {name}: {e}")
        return [TextContent(type="text", text=f"Error: {str(e)}")]

def _get_llm_search_instance(model: str = None, engine: str = None):
    """è·å–LLMæœç´¢å®ä¾‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼"""
    global llm_search_instances

    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
    if model is None:
        model = SERVER_CONFIG["default_model"]
    if engine is None:
        engine = SERVER_CONFIG["default_engine"]

    infer_type = SERVER_CONFIG["default_infer_type"]
    each_query_result = SERVER_CONFIG["default_each_query_result"]

    key = f"{model}_{engine}_{infer_type}"
    if key not in llm_search_instances:
        # ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼ˆé‡æ–°åŠ è½½é…ç½®ä»¥é˜²ä¸‡ä¸€ï¼‰
        import os

        # é‡æ–°ç¡®è®¤ç¯å¢ƒå˜é‡è®¾ç½®
        serpapi_key = os.environ.get("SERPAPI_KEY")
        bing_key = os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY")

        logger.info(f"ğŸ”‘ åˆ›å»ºLLM_searchå®ä¾‹å‰çš„ç¯å¢ƒå˜é‡æ£€æŸ¥:")
        logger.info(f"   SERPAPI_KEY: {'å·²è®¾ç½®' if serpapi_key else 'æœªè®¾ç½®'}")
        if serpapi_key:
            logger.info(f"   SERPAPI_KEYå€¼: {serpapi_key[:10]}...")
        logger.info(f"   BING_SEARCH_V7_SUBSCRIPTION_KEY: {'å·²è®¾ç½®' if bing_key else 'æœªè®¾ç½®'}")

        # å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå°è¯•é‡æ–°ä»é…ç½®æ–‡ä»¶åŠ è½½
        if not serpapi_key and not bing_key:
            logger.warning("âš ï¸ ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå°è¯•é‡æ–°ä»é…ç½®æ–‡ä»¶åŠ è½½...")
            try:
                env_config_path = os.path.join(os.path.dirname(__file__), '..', '..', 'config', 'environment_config.json')
                with open(env_config_path, 'r', encoding='utf-8') as f:
                    env_config = json.load(f)

                search_engines = env_config.get("api_keys", {}).get("search_engines", {})
                if search_engines.get("serpapi_key"):
                    os.environ["SERPAPI_KEY"] = search_engines["serpapi_key"]
                    logger.info(f"ğŸ”„ é‡æ–°è®¾ç½®SERPAPI_KEY: {search_engines['serpapi_key'][:10]}...")

                if search_engines.get("bing_subscription_key"):
                    os.environ["BING_SEARCH_V7_SUBSCRIPTION_KEY"] = search_engines["bing_subscription_key"]
                    logger.info(f"ğŸ”„ é‡æ–°è®¾ç½®BING_SEARCH_V7_SUBSCRIPTION_KEY")

            except Exception as reload_e:
                logger.error(f"âŒ é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {reload_e}")

        try:
            logger.info(f"ğŸš€ å¼€å§‹åˆ›å»ºLLM_searchå®ä¾‹: {key}")
            llm_search_instances[key] = LLM_search(
                model=model,
                infer_type=infer_type,
                engine=engine,
                each_query_result=each_query_result
            )
            logger.info(f"âœ… LLM_searchå®ä¾‹åˆ›å»ºæˆåŠŸ: {key}")
        except Exception as e:
            logger.error(f"âŒ LLM_searchå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
            logger.error(f"   æ¨¡å‹: {model}, æ¨ç†ç±»å‹: {infer_type}, å¼•æ“: {engine}")
            # è¾“å‡ºæ›´è¯¦ç»†çš„ç¯å¢ƒå˜é‡ä¿¡æ¯ç”¨äºè°ƒè¯•
            logger.error(f"   å½“å‰SERPAPI_KEY: {os.environ.get('SERPAPI_KEY', 'None')}")
            logger.error(f"   å½“å‰BING_KEY: {os.environ.get('BING_SEARCH_V7_SUBSCRIPTION_KEY', 'None')}")
            raise e
    else:
        logger.info(f"â™»ï¸ å¤ç”¨å·²å­˜åœ¨çš„LLM_searchå®ä¾‹: {key}")

    return llm_search_instances[key]

async def _generate_search_queries(topic: str, description: str = "", model: str = None) -> Dict[str, Any]:
    logger.info(f"Generating search queries for topic: {topic}")

    try:
        if model is None:
            model = SERVER_CONFIG["default_model"]

        infer_type = SERVER_CONFIG.get("default_infer_type", "OpenAI")

        llm_search = _get_llm_search_instance(model=model)

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„get_queriesæ–¹æ³•
        import asyncio
        import functools
        loop = asyncio.get_event_loop()

        # ä»é…ç½®ä¸­è·å–æŸ¥è¯¢æ•°é‡
        query_count = SERVER_CONFIG.get("default_query_count", 30)

        # ä½¿ç”¨functools.partialæ¥ä¼ é€’å…³é”®å­—å‚æ•°
        get_queries_func = functools.partial(
            llm_search.get_queries,
            topic=topic,
            description=description,
            query_count=query_count
        )

        # æ·»åŠ è¶…æ—¶æœºåˆ¶ï¼Œé˜²æ­¢LLMè°ƒç”¨å¡ä½
        try:
            queries = await asyncio.wait_for(
                loop.run_in_executor(None, get_queries_func),
                timeout=SERVER_CONFIG.get("llm_request_timeout", 30)  # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            )
        except asyncio.TimeoutError:
            logger.error(f"LLM query generation timed out for topic: {topic}")
            # è¿”å›é»˜è®¤æŸ¥è¯¢
            queries = [
                topic,
                f"{topic} research",
                f"{topic} analysis",
                f"{topic} study"
            ]

        # ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶
        query_file_path = _save_queries_to_file(queries, topic, description)

        result = {
            "topic": topic,
            "description": description,
            "model": model,
            "queries": queries,
            "query_count": len(queries),
            "query_file_path": query_file_path,  # æ·»åŠ æ–‡ä»¶è·¯å¾„
            "processing_metadata": {
                "model": model,
                "method": "llm_generation",
                "timestamp": "2025-01-23",
                "query_file_saved": query_file_path is not None
            }
        }

        return result

    except Exception as e:
        logger.error(f"Error generating queries: {e}")
        print(e)
        return
async def _web_search(query_file_path: str = None, topic: str = "", top_n: int = None, engine: str = None) -> Dict[str, Any]:

    # ä»æ–‡ä»¶ä¸­è¯»å–æŸ¥è¯¢åˆ—è¡¨
    queries = _load_queries_from_file(query_file_path, topic)
    logger.info(f"Performing web search for {len(queries)} queries")

    try:
        if top_n is None:
            top_n = SERVER_CONFIG["default_total_urls"]  # ä¿®æ­£ï¼šweb searché˜¶æ®µåº”è¯¥ä½¿ç”¨total_urlsé…ç½®
        if engine is None:
            engine = SERVER_CONFIG["default_engine"]

        # è·å–LLMæœç´¢å®ä¾‹ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
        try:
            llm_search = _get_llm_search_instance(engine=engine)
            if llm_search is None:
                raise ValueError("Failed to create LLM_search instance")
        except Exception as e:
            logger.error(f"Failed to get LLM_search instance: {e}")
            return {
                "topic": topic,
                "queries": queries,
                "engine": engine or SERVER_CONFIG["default_engine"],
                "urls": [],
                "url_count": 0,
                "top_n": top_n,
                "processing_metadata": {
                    "engine": engine,
                    "query_count": len(queries),
                    "result_count": 0,
                    "method": "error_fallback",
                    "error": str(e)
                }
            }

        # ç›´æ¥è°ƒç”¨batch_web_searchï¼Œé¿å…çº¿ç¨‹æ± åµŒå¥—é—®é¢˜
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œbatch_web_searchï¼ŒæŸ¥è¯¢æ•°é‡: {len(queries)}")
            urls = llm_search.batch_web_search(queries, topic, top_n)
            logger.info(f"âœ… batch_web_searchå®Œæˆï¼Œè¿”å›URLæ•°é‡: {len(urls)}")
        except Exception as e:
            logger.error(f"âŒ batch_web_searchæ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            urls = []
        # ç›®å‰çš„é—®é¢˜ï¼š
        '''
            1. promptå’Œconfigçš„ç»Ÿä¸€åŠå”¯ä¸€ç®¡ç†
            2. ç²¾ç®€æ‰æ²¡å¿…è¦
            3. analyseè®©llmè‡ªåŠ¨åˆ¤æ–­,æ¯ä¸€ä¸ªå·¥å…·åé¦ˆç»™
        '''
        # ä¿å­˜URLç»“æœåˆ°æœ¬åœ°æ–‡ä»¶
        url_file_path = _save_urls_to_file(urls, topic, queries)

        result = {
            "topic": topic,
            "queries": queries,
            "engine": engine,
            "urls": urls,
            "url_count": len(urls),
            "url_file_path": url_file_path,  # æ·»åŠ æ–‡ä»¶è·¯å¾„
            "top_n": top_n,
            "processing_metadata": {
                "engine": engine,
                "query_count": len(queries),
                "result_count": len(urls),
                "method": "batch_web_search",
                "url_file_saved": url_file_path is not None
            }
        }

        return result

    except Exception as e:
        logger.error(f"Error in web search: {e}")
        # å³ä½¿å‡ºé”™ä¹Ÿå°è¯•ä¿å­˜ç©ºçš„URLæ–‡ä»¶
        url_file_path = _save_urls_to_file([], topic, queries)

        return {
            "topic": topic,
            "queries": queries,
            "engine": engine or SERVER_CONFIG["default_engine"],
            "urls": [],
            "url_count": 0,
            "url_file_path": url_file_path,
            "top_n": top_n or SERVER_CONFIG["default_total_urls"],
            "processing_metadata": {
                "engine": engine or SERVER_CONFIG["default_engine"],
                "query_count": len(queries),
                "result_count": 0,
                "method": "fallback",
                "error": str(e),
                "url_file_saved": url_file_path is not None
            }
        }

async def _crawl_urls(topic: str, url_file_path: str = None, top_n: int = None, model: str = None,
                     similarity_threshold: float = None, min_length: int = None, max_length: int = None) -> Dict[str, Any]:

    # ä»æ–‡ä»¶ä¸­è¯»å–URLåˆ—è¡¨
    url_list = _load_urls_from_file(url_file_path, topic)
    logger.info(f"Starting crawling process for {len(url_list)} URLs with topic: {topic}")

    try:
        if top_n is None:
            top_n = SERVER_CONFIG["default_top_n"]
        if model is None:
            model = SERVER_CONFIG["default_model"]
        if similarity_threshold is None:
            similarity_threshold = SERVER_CONFIG["default_similarity_threshold"]
        if min_length is None:
            min_length = SERVER_CONFIG["default_min_length"]
        if max_length is None:
            max_length = SERVER_CONFIG["default_max_length"]

        if not CRAWL4AI_AVAILABLE:
            raise ImportError("crawl4ai is not available")
        if RequestWrapper is None:
            raise ImportError("RequestWrapper is not available")

        import time
        import re

        # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹è®¾ç½®
        content_model = SERVER_CONFIG.get("content_analysis_model", model)
        infer_type = SERVER_CONFIG.get("default_infer_type", "OpenAI")
        request_wrapper = RequestWrapper(model=content_model, infer_type=infer_type)

        process_start_time = time.time()
        stage_time = process_start_time

        # çˆ¬å–URLï¼Œä¸é™åˆ¶æ•´ä½“è¶…æ—¶æ—¶é—´
        crawling_timeout = SERVER_CONFIG.get("crawling_timeout", 0)  # 0è¡¨ç¤ºä¸é™åˆ¶
        if crawling_timeout > 0:
            logger.info(f"å¼€å§‹çˆ¬å– {len(url_list)} ä¸ªURLï¼Œè¶…æ—¶æ—¶é—´: {crawling_timeout}ç§’")
        else:
            logger.info(f"å¼€å§‹çˆ¬å– {len(url_list)} ä¸ªURLï¼Œæ— æ•´ä½“è¶…æ—¶é™åˆ¶")

        try:
            if crawling_timeout > 0:
                crawl_results = await asyncio.wait_for(
                    _crawl_urls_stage(topic, url_list),
                    timeout=crawling_timeout
                )
            else:
                # ä¸é™åˆ¶æ•´ä½“è¶…æ—¶æ—¶é—´
                crawl_results = await _crawl_urls_stage(topic, url_list)
            logger.info(f"Stage 1 - Crawling completed in {time.time() - stage_time:.2f} seconds, with {len(crawl_results)} results")
        except asyncio.TimeoutError:
            logger.error(f"URL crawling stage timed out after {crawling_timeout} seconds")
            logger.error("âš ï¸ Attempting to retrieve partial results from incremental save file...")

            # å°è¯•ä»å¢é‡ä¿å­˜æ–‡ä»¶ä¸­æ¢å¤éƒ¨åˆ†ç»“æœ
            try:
                import os
                import json
                incremental_file_path = _get_incremental_crawl_file_path(topic)
                logger.info(f"å°è¯•ä»å¢é‡æ–‡ä»¶æ¢å¤ç»“æœ: {incremental_file_path}")

                if os.path.exists(incremental_file_path):
                    with open(incremental_file_path, 'r', encoding='utf-8') as f:
                        incremental_data = json.load(f)

                    crawl_progress = incremental_data.get("crawl_progress", [])
                    logger.info(f"ä»å¢é‡æ–‡ä»¶ä¸­æ‰¾åˆ° {len(crawl_progress)} ä¸ªå·²å®Œæˆçš„çˆ¬å–ç»“æœ")

                    # é‡æ„å¢é‡ç»“æœä¸ºæ ‡å‡†æ ¼å¼ï¼Œä½¿ç”¨ä¿å­˜çš„å®é™…å†…å®¹
                    crawl_results = []
                    for progress in crawl_progress:
                        if progress.get("success", False):
                            # è¿™æ˜¯æˆåŠŸçš„ç»“æœï¼Œä½¿ç”¨ä¿å­˜çš„å®é™…å†…å®¹
                            crawl_results.append({
                                "url": progress.get("url", ""),
                                "error": False,
                                "raw_content": progress.get("content", ""),  # ä½¿ç”¨ä¿å­˜çš„å®é™…å†…å®¹
                                "title": progress.get("title", ""),
                                "date": progress.get("date", ""),
                                "timestamp": progress.get("timestamp", ""),
                                "is_recovered": True  # æ ‡è®°ä¸ºä»å¢é‡æ–‡ä»¶æ¢å¤çš„ç»“æœ
                            })
                        else:
                            # è¿™æ˜¯é”™è¯¯ç»“æœ
                            crawl_results.append({
                                "url": progress.get("url", ""),
                                "error": True,
                                "raw_content": progress.get("error_message", "Unknown error"),
                                "timestamp": progress.get("timestamp", "")
                            })

                    logger.info(f"âœ… æˆåŠŸæ¢å¤ {len(crawl_results)} ä¸ªéƒ¨åˆ†ç»“æœ")
                else:
                    logger.error(f"å¢é‡æ–‡ä»¶ä¸å­˜åœ¨: {incremental_file_path}")
                    crawl_results = []

                # è¶…æ—¶åˆ†æ
                logger.error(f"ğŸ“Š Timeout analysis:")
                logger.error(f"  - Total URLs to crawl: {len(url_list)}")
                logger.error(f"  - Completed URLs: {len(crawl_results)}")
                logger.error(f"  - Completion rate: {len(crawl_results)/len(url_list)*100:.1f}%")
                logger.error(f"  - Timeout setting: {crawling_timeout} seconds")
                logger.error(f"  - Average time per URL: {crawling_timeout / len(url_list):.2f} seconds")

                # å»ºè®®è°ƒæ•´
                suggested_timeout = len(url_list) * 60  # æ¯ä¸ªURLç»™60ç§’
                logger.error(f"  - Suggested timeout: {suggested_timeout} seconds")

            except Exception as e:
                logger.error(f"Error during partial result recovery: {e}")
                crawl_results = []
        stage_time = time.time()

        # é‡æ–°å¯ç”¨å†…å®¹è¿‡æ»¤å’Œç›¸ä¼¼åº¦è¯„åˆ†ï¼Œä½†ä½¿ç”¨å®½æ¾è®¾ç½®
        logger.info("Stage 2 - Content filtering with relaxed settings")

        # ç®€åŒ–çš„å†…å®¹è¿‡æ»¤ï¼šåªæ·»åŠ åŸºæœ¬ä¿¡æ¯ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹
        filtered_results = []
        error_count = 0
        success_count = 0

        for result in crawl_results:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ç»“æœ
            is_error = result.get("error", False)

            # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
            content = result.get("cleaned_content", "") or result.get("raw_content", "") or result.get("content", "")

            # è·å–æ¸…ç†ä¿¡æ¯
            cleaning_info = result.get("cleaning_info", {})
            title = result.get("title", "")
            language = result.get("language", "UNKNOWN")

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            if is_error:
                error_count += 1
                logger.warning(f"Error result for URL {result.get('url', 'unknown')}: {content[:100]}...")
            else:
                success_count += 1

            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ åŸºæœ¬ä¿¡æ¯ï¼Œä¿ç•™æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬é”™è¯¯ç»“æœç”¨äºè°ƒè¯•ï¼‰
            enhanced_result = {
                "url": result.get("url", ""),
                "title": title or result.get("title", "Unknown"),  # ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜
                "content": content,  # ä½¿ç”¨æ¸…ç†åçš„å†…å®¹
                "raw_content": result.get("raw_content", ""),  # ä¿ç•™åŸå§‹å†…å®¹
                "date": result.get("date", ""),
                "length": len(content),
                "language": language,  # æ·»åŠ è¯­è¨€ä¿¡æ¯
                "is_error": is_error,
                "original_error": result.get("error", False),
                "cleaning_info": cleaning_info  # æ·»åŠ æ¸…ç†ä¿¡æ¯
            }
            filtered_results.append(enhanced_result)

        logger.info(f"Stage 2 completed - processed {len(filtered_results)} results (success: {success_count}, errors: {error_count})")

        # å¦‚æœæ‰€æœ‰ç»“æœéƒ½æ˜¯é”™è¯¯ï¼Œè®°å½•è­¦å‘Š
        if error_count > 0 and success_count == 0:
            logger.warning(f"âš ï¸ All {error_count} crawl results failed! This will likely result in 0 final results.")
        elif error_count > 0:
            logger.warning(f"âš ï¸ {error_count} out of {len(crawl_results)} crawl results failed.")
        stage_time = time.time()

        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è¯„åˆ†ï¼šç»™æ‰€æœ‰ç»“æœé«˜åˆ†
        logger.info("Stage 3 - Simplified similarity scoring")
        scored_results = []
        for result in filtered_results:
            result["similarity_score"] = 90.0  # ç»™æ‰€æœ‰ç»“æœé«˜åˆ†ï¼Œç¡®ä¿é€šè¿‡è¿‡æ»¤
            scored_results.append(result)

        logger.info(f"Stage 3 completed - scored {len(scored_results)} results")
        stage_time = time.time()

        # æš‚æ—¶è·³è¿‡è¿‡æ»¤ï¼Œç›´æ¥è¿”å›æ‰€æœ‰ç»“æœç”¨äºæµ‹è¯•
        print(f"DEBUG: scored_results count: {len(scored_results)}")
        if scored_results:
            print(f"DEBUG: First result keys: {list(scored_results[0].keys())}")
            print(f"DEBUG: First result similarity_score: {scored_results[0].get('similarity_score', 'N/A')}")
            print(f"DEBUG: First result content length: {len(scored_results[0].get('content', ''))}")
            print(f"DEBUG: First result is_error: {scored_results[0].get('is_error', 'N/A')}")
            print(f"DEBUG: First result content preview: {scored_results[0].get('content', '')[:200]}...")

        # æ³¨é‡Šæ‰é”™è¯¯è¿‡æ»¤å’Œé”™è¯¯ä¿¡æ¯è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰ç»“æœç”¨äºè°ƒè¯•
        valid_results = []
        for result in scored_results:
            # æ³¨é‡Šæ‰é”™è¯¯è¿‡æ»¤ï¼šif not result.get("is_error", False):
            # è¿›ä¸€æ­¥æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            content = result.get("content", "")
            # æ³¨é‡Šæ‰é”™è¯¯ä¿¡æ¯è¿‡æ»¤ï¼šif content and not content.startswith("Error:") and len(content.strip()) > 50:
            if content and len(content.strip()) > 0:  # è¿›ä¸€æ­¥é™ä½è¦æ±‚ï¼šåªè¦æœ‰å†…å®¹å°±ä¿ç•™
                valid_results.append(result)
                logger.info(f"Kept result: {result.get('url', 'unknown')}, content_length={len(content)}, stripped_length={len(content.strip())}")
            else:
                logger.warning(f"Filtered out result with invalid content: {result.get('url', 'unknown')}, content_length={len(content)}, stripped_length={len(content.strip()) if content else 0}")
            # æ³¨é‡Šæ‰é”™è¯¯ç»“æœè¿‡æ»¤ï¼šelse:
            #     logger.warning(f"Filtered out error result: {result.get('url', 'unknown')}")

        print(f"DEBUG: valid_results count after content filtering: {len(valid_results)}")

        # æ·»åŠ è¯¦ç»†çš„å†…å®¹åˆ†æ
        if len(valid_results) > 0:
            first_result = valid_results[0]
            content = first_result.get("content", "")
            print(f"DEBUG: First valid result analysis:")
            print(f"  - URL: {first_result.get('url', 'unknown')}")
            print(f"  - Original length: {len(content)}")
            print(f"  - Stripped length: {len(content.strip())}")
            print(f"  - Is error: {first_result.get('is_error', False)}")
            print(f"  - Content preview (first 200 chars): '{content[:200]}...'")
            print(f"  - Content preview (last 200 chars): '...{content[-200:]}'")

            # åˆ†æå†…å®¹ç»„æˆ
            import re
            whitespace_count = len(re.findall(r'\s', content))
            meaningful_chars = len(re.findall(r'[a-zA-Z0-9\u4e00-\u9fff]', content))
            print(f"  - Whitespace characters: {whitespace_count}")
            print(f"  - Meaningful characters: {meaningful_chars}")

        # ä½¿ç”¨valid_resultsä½œä¸ºfinal_results
        final_results = valid_results[:top_n]  # åªå–å‰top_nä¸ªç»“æœ
        print(f"DEBUG: final_results count: {len(final_results)}")
        logger.info(f"Stage 4 - Result processing completed in {time.time() - stage_time:.2f} seconds")

        # å¦‚æœæœ€ç»ˆç»“æœä¸º0ï¼Œè®°å½•è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        if len(final_results) == 0:
            logger.error("ğŸš¨ FINAL RESULTS IS ZERO! Debugging information:")
            logger.error(f"  - Original crawl_results: {len(crawl_results)}")
            logger.error(f"  - Filtered_results: {len(filtered_results)}")
            logger.error(f"  - Scored_results: {len(scored_results)}")
            logger.error(f"  - Valid_results: {len(valid_results)}")
            logger.error(f"  - Success_count: {success_count}")
            logger.error(f"  - Error_count: {error_count}")

            # æ˜¾ç¤ºå‰å‡ ä¸ªåŸå§‹ç»“æœçš„è¯¦ç»†ä¿¡æ¯
            for i, result in enumerate(crawl_results[:5]):
                raw_content = result.get('raw_content', '')
                logger.error(f"  - Crawl result {i+1}:")
                logger.error(f"    URL: {result.get('url', 'N/A')}")
                logger.error(f"    Error: {result.get('error', False)}")
                logger.error(f"    Content length: {len(raw_content)}")
                logger.error(f"    Stripped length: {len(raw_content.strip()) if raw_content else 0}")
                if raw_content:
                    logger.error(f"    Content preview: '{raw_content[:200]}...'")
                    if raw_content.startswith("Error:"):
                        logger.error(f"    âš ï¸ This is an error result")
                    else:
                        logger.error(f"    âœ… This appears to be valid content")

            # åˆ†æä¸ºä»€ä¹ˆæ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤
            logger.error("ğŸ” Analysis of why all results were filtered:")
            all_errors = all(r.get("error", False) for r in crawl_results)
            all_empty = all(len(r.get("raw_content", "").strip()) == 0 for r in crawl_results)

            if all_errors:
                logger.error("  ğŸš¨ ALL results are error results")
            elif all_empty:
                logger.error("  ğŸš¨ ALL results have empty content after stripping")
            else:
                error_count_check = sum(1 for r in crawl_results if r.get("error", False))
                empty_count = sum(1 for r in crawl_results if len(r.get("raw_content", "").strip()) == 0)
                logger.error(f"  ğŸš¨ Mixed issues: {error_count_check} errors, {empty_count} empty content")
        else:
            logger.info(f"âœ… Successfully generated {len(final_results)} final results")

        total_time = time.time() - process_start_time
        logger.info(f"Total crawling process completed in {total_time:.2f} seconds")

        # ç”Ÿæˆå…¼å®¹ LLMxMapReduce_V2 JSONL æ ¼å¼çš„ç»“æœ
        # ä¸»è¦ç»“æœï¼šä¸»é¢˜+è®ºæ–‡åˆ—è¡¨çš„æ ¼å¼
        main_result = {
            "title": topic,
            "papers": final_results
        }

        # è¯¦ç»†çš„å¤„ç†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰
        detailed_result = {
            "title": topic,  # æ”¹ä¸ºtitle
            "total_urls": len(url_list),
            "crawl_results": len(crawl_results),
            "filtered_results": len(filtered_results),
            "scored_results": len(scored_results),
            "papers": final_results,  # æ”¹ä¸ºpapers
            "final_count": len(final_results),
            "processing_metadata": {
                "model": model,
                "similarity_threshold": similarity_threshold,
                "min_length": min_length,
                "max_length": max_length,
                "top_n": top_n,
                "total_time": total_time,
                "success": True
            },
            # æ·»åŠ å…¼å®¹æ ¼å¼çš„ä¸»è¦ç»“æœ
            "llm_mapreduce_format": main_result
        }

        # ä¿å­˜ç»“æœåˆ°æœ¬åœ°JSONæ–‡ä»¶ï¼ŒåŒ…å«è°ƒè¯•ä¿¡æ¯å’Œé”™è¯¯åˆ†æ
        try:
            import json
            import datetime

            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_results_{timestamp}.json"

            # åˆ†æé”™è¯¯ç±»å‹
            error_analysis = {
                "total_errors": error_count,
                "total_success": success_count,
                "error_details": [],
                "error_types": {}
            }

            # æ”¶é›†é”™è¯¯è¯¦æƒ…
            for result in crawl_results:
                if result.get("error", False):
                    error_detail = {
                        "url": result.get("url", "unknown"),
                        "error_message": result.get("raw_content", ""),
                        "timestamp": result.get("timestamp", "")
                    }
                    error_analysis["error_details"].append(error_detail)

                    # ç»Ÿè®¡é”™è¯¯ç±»å‹
                    error_msg = result.get("raw_content", "")
                    if "timeout" in error_msg.lower():
                        error_type = "timeout"
                    elif "connection" in error_msg.lower():
                        error_type = "connection"
                    elif "none" in error_msg.lower() and "attribute" in error_msg.lower():
                        error_type = "parsing_error"
                    elif "403" in error_msg or "forbidden" in error_msg.lower():
                        error_type = "access_denied"
                    elif "404" in error_msg or "not found" in error_msg.lower():
                        error_type = "not_found"
                    else:
                        error_type = "other"

                    error_analysis["error_types"][error_type] = error_analysis["error_types"].get(error_type, 0) + 1

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            debug_info = {
                "raw_crawl_results": crawl_results,
                "processed_filtered_results": filtered_results,
                "processed_scored_results": scored_results,
                "error_analysis": error_analysis
            }
            detailed_result["debug_info"] = debug_info

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(detailed_result, f, ensure_ascii=False, indent=2)

            logger.info(f"Results saved to {filename}")
            logger.info(f"Error analysis: {error_analysis['error_types']}")
            detailed_result["saved_file"] = filename

            # å•ç‹¬ä¿å­˜é”™è¯¯æŠ¥å‘Š
            if error_count > 0:
                error_filename = f"crawl_errors_{timestamp}.json"
                with open(error_filename, 'w', encoding='utf-8') as f:
                    json.dump(error_analysis, f, ensure_ascii=False, indent=2)
                logger.info(f"Error report saved to {error_filename}")
                detailed_result["error_report_file"] = error_filename

        except Exception as e:
            logger.error(f"Failed to save results to JSON: {e}")

        return detailed_result

    except Exception as e:
        logger.error(f"Error in crawling pipeline: {e}")
        return {
            "topic": topic,
            "total_urls": len(url_list),
            "crawl_results": 0,
            "filtered_results": 0,
            "scored_results": 0,
            "final_results": [],
            "final_count": 0,
            "processing_metadata": {
                "model": model,
                "similarity_threshold": similarity_threshold,
                "min_length": min_length,
                "max_length": max_length,
                "top_n": top_n,
                "success": False,
                "error": str(e)
            }
        }

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰ä¼šè¯çš„å¢é‡æ–‡ä»¶è·¯å¾„
_current_incremental_file_path = None

def _get_incremental_crawl_file_path(topic: str, create_new: bool = False) -> str:
    """è·å–å¢é‡çˆ¬å–ç»“æœæ–‡ä»¶è·¯å¾„"""
    global _current_incremental_file_path

    if _current_incremental_file_path is None or create_new:
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"crawl_results_{topic}_{timestamp}.json"
        _current_incremental_file_path = filename

    return _current_incremental_file_path

def _load_existing_crawl_results(file_path: str) -> Dict[str, Any]:
    """åŠ è½½ç°æœ‰çš„çˆ¬å–ç»“æœæ–‡ä»¶"""
    import json
    import os

    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load existing crawl results: {e}")

    # è¿”å›é»˜è®¤ç»“æ„
    return {
        "topic": "",
        "total_urls": 0,
        "crawl_results": 0,
        "filtered_results": 0,
        "scored_results": 0,
        "final_results": [],
        "crawl_progress": []
    }

async def _generate_abstract_llm(content: str, url: str, file_path: str, url_index: int) -> str:
    """
    å¼‚æ­¥ç”ŸæˆLLMæ‘˜è¦å¹¶æ›´æ–°æ–‡ä»¶

    Args:
        content: è¦ç”Ÿæˆæ‘˜è¦çš„å†…å®¹
        url: URLåœ°å€
        file_path: å¢é‡æ–‡ä»¶è·¯å¾„
        url_index: URLåœ¨crawl_progressä¸­çš„ç´¢å¼•

    Returns:
        ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
    """
    try:
        # è·å–abstractç”Ÿæˆæ¨¡å‹é…ç½®
        abstract_model = SERVER_CONFIG.get("abstract_generation_model", "gemini-2.5-flash")
        infer_type = SERVER_CONFIG.get("default_infer_type", "OpenAI")

        # åˆ›å»ºRequestWrapperå®ä¾‹
        request_wrapper = RequestWrapper(model=abstract_model, infer_type=infer_type)

        # è·å–promptæ¨¡æ¿
        prompts = SERVER_CONFIG.get("prompts", {})
        prompt_template = prompts.get("abstract_generation", "Please summarize the following content:\n\n{content}")

        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…tokenè¿‡å¤š
        max_content_length = 8000  # çº¦2000ä¸ªtoken
        if len(content) > max_content_length:
            content = content[:max_content_length] + "..."

        # ç”Ÿæˆprompt
        prompt = prompt_template.format(content=content)

        # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦ï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
        logger.info(f"å¼€å§‹ä¸ºURLç”ŸæˆLLMæ‘˜è¦: {url}")
        abstract_timeout = SERVER_CONFIG.get("abstract_generation_timeout", 30)

        try:
            # ä½¿ç”¨asyncio.to_threadå°†åŒæ­¥è°ƒç”¨è½¬ä¸ºå¼‚æ­¥ï¼Œå¹¶æ·»åŠ è¶…æ—¶
            abstract_llm = await asyncio.wait_for(
                asyncio.to_thread(request_wrapper.completion, prompt),
                timeout=abstract_timeout
            )
        except asyncio.TimeoutError:
            logger.error(f"LLMæ‘˜è¦ç”Ÿæˆè¶…æ—¶ ({abstract_timeout}ç§’): {url}")
            raise Exception(f"Abstract generation timeout after {abstract_timeout} seconds")

        # æ¸…ç†ç”Ÿæˆçš„æ‘˜è¦
        abstract_llm = abstract_llm.strip()
        if len(abstract_llm) > 1000:  # é™åˆ¶æ‘˜è¦é•¿åº¦
            abstract_llm = abstract_llm[:1000] + "..."

        # æ›´æ–°æ–‡ä»¶ä¸­çš„abstract_llmå­—æ®µ
        await _update_abstract_llm_in_file(file_path, url_index, abstract_llm, "completed")

        logger.info(f"âœ… LLMæ‘˜è¦ç”Ÿæˆå®Œæˆ: {url}")
        return abstract_llm

    except Exception as e:
        logger.error(f"âŒ LLMæ‘˜è¦ç”Ÿæˆå¤±è´¥ {url}: {e}")
        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
        await _update_abstract_llm_in_file(file_path, url_index, f"Error: {str(e)}", "failed")
        return f"Error: {str(e)}"

async def _update_abstract_llm_in_file(file_path: str, url_index: int, abstract_llm: str, status: str):
    """
    æ›´æ–°æ–‡ä»¶ä¸­æŒ‡å®šURLçš„abstract_llmå­—æ®µ

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        url_index: URLåœ¨crawl_progressä¸­çš„ç´¢å¼•
        abstract_llm: LLMç”Ÿæˆçš„æ‘˜è¦
        status: çŠ¶æ€ï¼ˆcompleted/failedï¼‰
    """
    import json
    import os
    import time
    import platform

    # è·¨å¹³å°æ–‡ä»¶é”å®ç°
    def _acquire_file_lock(f):
        if platform.system() == "Windows":
            # Windowsä¸‹ä½¿ç”¨msvcrt
            try:
                import msvcrt
                msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
            except ImportError:
                # å¦‚æœmsvcrtä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„é‡è¯•æœºåˆ¶
                pass
        else:
            # Unix/Linuxä¸‹ä½¿ç”¨fcntl
            try:
                import fcntl
                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            except ImportError:
                pass

    def _release_file_lock(f):
        if platform.system() == "Windows":
            try:
                import msvcrt
                msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
            except ImportError:
                pass
        else:
            try:
                import fcntl
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
            except ImportError:
                pass

    max_retries = 3  # å‡å°‘é‡è¯•æ¬¡æ•°
    retry_delay = 0.05  # å‡å°‘åˆå§‹å»¶è¿Ÿ

    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨æ–‡ä»¶é”é¿å…å¹¶å‘å†™å…¥å†²çª
            with open(file_path, 'r+', encoding='utf-8') as f:
                # è·å–æ–‡ä»¶é”
                _acquire_file_lock(f)

                try:
                    # è¯»å–ç°æœ‰æ•°æ®
                    f.seek(0)
                    data = json.load(f)

                    # æ›´æ–°æŒ‡å®šç´¢å¼•çš„abstract_llmå­—æ®µ
                    if url_index < len(data.get("crawl_progress", [])):
                        data["crawl_progress"][url_index]["abstract_llm"] = abstract_llm
                        data["crawl_progress"][url_index]["abstract_llm_status"] = status

                        # å†™å›æ–‡ä»¶
                        f.seek(0)
                        f.truncate()
                        json.dump(data, f, ensure_ascii=False, indent=2)

                        logger.debug(f"æ›´æ–°abstract_llmå­—æ®µæˆåŠŸï¼Œç´¢å¼•: {url_index}")
                        return  # æˆåŠŸæ›´æ–°ï¼Œé€€å‡ºå‡½æ•°
                    else:
                        logger.error(f"URLç´¢å¼•è¶…å‡ºèŒƒå›´: {url_index}")
                        return

                finally:
                    # é‡Šæ”¾æ–‡ä»¶é”
                    _release_file_lock(f)

        except (json.JSONDecodeError, OSError, IOError) as e:
            if attempt < max_retries - 1:
                logger.warning(f"æ›´æ–°abstract_llmå­—æ®µå¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {e}")
                await asyncio.sleep(retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"æ›´æ–°abstract_llmå­—æ®µæœ€ç»ˆå¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"æ›´æ–°abstract_llmå­—æ®µå‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")
            break

def _save_incremental_crawl_result(file_path: str, crawl_result: Dict[str, Any], topic: str, total_urls: int) -> int:
    """å¢é‡ä¿å­˜å•ä¸ªçˆ¬å–ç»“æœï¼ŒåŒ…å«HTMLæ¸…ç†ä¿¡æ¯

    Returns:
        int: æ–°å¢è®°å½•åœ¨crawl_progressä¸­çš„ç´¢å¼•
    """
    import json
    import os

    try:
        # åŠ è½½ç°æœ‰ç»“æœ
        existing_data = _load_existing_crawl_results(file_path)

        # æ›´æ–°åŸºæœ¬ä¿¡æ¯
        existing_data["topic"] = topic
        existing_data["total_urls"] = total_urls
        existing_data["crawl_results"] = len(existing_data["crawl_progress"]) + 1

        # è·å–å†…å®¹ä¿¡æ¯
        raw_content = crawl_result.get("raw_content", "")
        cleaned_content = crawl_result.get("cleaned_content", "")

        # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹ä½œä¸ºä¸»è¦å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
        main_content = cleaned_content if cleaned_content else raw_content

        # è·å–æ¸…ç†ä¿¡æ¯
        cleaning_info = crawl_result.get("cleaning_info", {})

        # ç”Ÿæˆabstractå­—æ®µï¼ˆå–å‰2000ä¸ªå­—ç¬¦ï¼‰
        abstract = ""
        if main_content and not crawl_result.get("error", False):
            # æ¸…ç†å†…å®¹å¹¶æˆªå–å‰2000å­—ç¬¦
            cleaned_for_abstract = main_content.strip()
            if len(cleaned_for_abstract) > 2000:
                # å°è¯•åœ¨å¥å·å¤„æˆªæ–­ï¼Œé¿å…æˆªæ–­å¥å­
                truncated = cleaned_for_abstract[:2000]
                last_period = truncated.rfind('.')
                if last_period > 1600:  # å¦‚æœå¥å·ä½ç½®åˆç†ï¼ˆè‡³å°‘80%çš„å†…å®¹ï¼‰
                    abstract = truncated[:last_period + 1]
                else:
                    abstract = truncated + "..."
            else:
                abstract = cleaned_for_abstract

        # æ„å»ºä¿å­˜æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„HTMLæ¸…ç†ä¿¡æ¯å’Œæ–°å¢çš„abstractå­—æ®µ
        save_data = {
            "url": crawl_result.get("url", ""),
            "success": not crawl_result.get("error", False),
            "content_length": len(main_content),
            "timestamp": crawl_result.get("timestamp", ""),
            "error_message": raw_content if crawl_result.get("error", False) else "",
            "content": main_content,  # ä¸»è¦å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„ï¼‰
            "raw_content": raw_content,  # ä¿ç•™åŸå§‹å†…å®¹ç”¨äºå¯¹æ¯”
            "cleaned_content": cleaned_content,  # æ¸…ç†åçš„å†…å®¹
            "title": crawl_result.get("title", ""),
            "language": crawl_result.get("language", "UNKNOWN"),
            "date": crawl_result.get("date", ""),
            "cleaning_info": cleaning_info,  # HTMLæ¸…ç†å…ƒæ•°æ®
            "abstract": abstract,  # æ–°å¢ï¼šå†…å®¹å‰2000å­—ç¬¦çš„æ‘˜è¦
            "abstract_llm": "",  # æ–°å¢ï¼šLLMç”Ÿæˆçš„æ‘˜è¦ï¼ˆåˆå§‹ä¸ºç©ºï¼Œåç»­å¼‚æ­¥å¡«å……ï¼‰
            "abstract_llm_status": "pending"  # æ–°å¢ï¼šLLMæ‘˜è¦ç”ŸæˆçŠ¶æ€
        }

        # è·å–æ–°è®°å½•çš„ç´¢å¼•ï¼ˆåœ¨æ·»åŠ ä¹‹å‰ï¼‰
        new_index = len(existing_data["crawl_progress"])

        existing_data["crawl_progress"].append(save_data)

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)

        # è®°å½•æ¸…ç†æ•ˆæœ
        if cleaning_info.get("method") != "none":
            logger.info(f"Saved crawl result with {cleaning_info.get('method')} cleaning for URL: {crawl_result.get('url', 'N/A')}")
        else:
            logger.debug(f"Saved crawl result (no cleaning) for URL: {crawl_result.get('url', 'N/A')}")

        return new_index

    except Exception as e:
        logger.error(f"Failed to save incremental crawl result: {e}")
        return -1  # è¿”å›-1è¡¨ç¤ºä¿å­˜å¤±è´¥

async def _crawl_urls_stage(topic: str, url_list: List[str]) -> List[Dict[str, Any]]:
    import asyncio

    if not CRAWL4AI_AVAILABLE:
        raise ImportError("crawl4ai is not available")

    MAX_CONCURRENT_CRAWLS = SERVER_CONFIG.get("max_concurrent_crawls", 10)  # ä»é…ç½®è¯»å–å¹¶å‘æ•°é‡
    input_queue = asyncio.Queue()
    output_queue = asyncio.Queue()
    total_items = len(url_list)

    # è·Ÿè¸ªabstractç”Ÿæˆä»»åŠ¡
    abstract_tasks = []

    # åˆ›å»ºå¢é‡ä¿å­˜æ–‡ä»¶
    incremental_file_path = _get_incremental_crawl_file_path(topic, create_new=True)
    logger.info(f"å¢é‡çˆ¬å–ç»“æœå°†ä¿å­˜åˆ°: {incremental_file_path}")

    # æ·»åŠ URLåˆ°é˜Ÿåˆ—
    for url in url_list:
        await input_queue.put((url, topic))

    async def consumer():
        consumer_id = id(asyncio.current_task())
        processed_count = 0

        while True:
            try:
                url, topic = input_queue.get_nowait()
                try:
                    result = await _crawl_single_url(url, topic)

                    # æ·»åŠ æ—¶é—´æˆ³
                    import datetime
                    result["timestamp"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                    # ç«‹å³ä¿å­˜åˆ°å¢é‡æ–‡ä»¶ï¼Œè·å–ç´¢å¼•
                    url_index = _save_incremental_crawl_result(incremental_file_path, result, topic, total_items)

                    await output_queue.put(result)
                    processed_count += 1

                    # å¦‚æœçˆ¬å–æˆåŠŸä¸”æœ‰å†…å®¹ï¼Œå¼‚æ­¥å¯åŠ¨abstract_llmç”Ÿæˆ
                    if (not result.get("error", False) and
                        url_index >= 0 and
                        result.get("cleaned_content") or result.get("raw_content")):

                        content_for_abstract = result.get("cleaned_content") or result.get("raw_content", "")
                        if content_for_abstract.strip():
                            # å¼‚æ­¥å¯åŠ¨abstract_llmç”Ÿæˆï¼Œå¹¶æ·»åŠ åˆ°è·Ÿè¸ªåˆ—è¡¨
                            task = asyncio.create_task(_generate_abstract_llm(
                                content_for_abstract,
                                url,
                                incremental_file_path,
                                url_index
                            ))
                            abstract_tasks.append(task)

                    # å‡å°‘æ—¥å¿—é¢‘ç‡ï¼Œåªåœ¨å¤„ç†æ¯10ä¸ªURLæˆ–é˜Ÿåˆ—ä¸ºç©ºæ—¶è®°å½•
                    remaining = input_queue.qsize()
                    if processed_count % 10 == 0 or remaining == 0:
                        logger.info(f"Consumer {consumer_id}: å·²å¤„ç† {processed_count} ä¸ªURL, å‰©ä½™: {remaining}/{total_items}")

                except Exception as e:
                    logger.error(f"Consumer {consumer_id}: çˆ¬å–URLå¤±è´¥ {url}: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦æ”¾å…¥ç»“æœï¼Œé¿å…ä¸¢å¤±
                    import datetime
                    error_result = {
                        "topic": topic,
                        "url": url,
                        "raw_content": f"Error: {e}",
                        "error": True,
                        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }

                    # ä¿å­˜é”™è¯¯ç»“æœ
                    error_index = _save_incremental_crawl_result(incremental_file_path, error_result, topic, total_items)
                    await output_queue.put(error_result)
                finally:
                    input_queue.task_done()
            except asyncio.QueueEmpty:
                if processed_count > 0:
                    logger.info(f"Consumer {consumer_id}: å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªURL")
                break

    consumers = [asyncio.create_task(consumer()) for _ in range(MAX_CONCURRENT_CRAWLS)]
    logger.info(f"å¯åŠ¨ {MAX_CONCURRENT_CRAWLS} ä¸ªå¹¶å‘çˆ¬å–ä»»åŠ¡å¤„ç† {total_items} ä¸ªURL")

    # æ·»åŠ è¿›åº¦ç›‘æ§å’Œé˜Ÿåˆ—ç­‰å¾…è¶…æ—¶ä¿æŠ¤
    start_time = time.time()
    try:
        # è®¡ç®—é˜Ÿåˆ—ç­‰å¾…è¶…æ—¶ï¼šæ¯ä¸ªURLç»™60ç§’ + 5åˆ†é’Ÿç¼“å†²
        queue_timeout = len(url_list) * 60 + 300
        logger.info(f"é˜Ÿåˆ—ç­‰å¾…è¶…æ—¶è®¾ç½®: {queue_timeout}ç§’")

        await asyncio.wait_for(input_queue.join(), timeout=queue_timeout)
        end_time = time.time()
        logger.info(f"æ‰€æœ‰URLçˆ¬å–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    except asyncio.TimeoutError:
        logger.error(f"é˜Ÿåˆ—ç­‰å¾…è¶…æ—¶ ({queue_timeout}ç§’)ï¼Œå¼ºåˆ¶ç»“æŸçˆ¬å–æµç¨‹")
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
    finally:
        # ç¡®ä¿æ‰€æœ‰consumerä»»åŠ¡è¢«æ­£ç¡®å–æ¶ˆ
        for consumer_task in consumers:
            if not consumer_task.done():
                consumer_task.cancel()

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå–æ¶ˆ
        await asyncio.gather(*consumers, return_exceptions=True)

        # ç­‰å¾…abstractç”Ÿæˆä»»åŠ¡å®Œæˆï¼ˆå¸¦è¶…æ—¶ï¼‰
        if abstract_tasks:
            abstract_wait_timeout = SERVER_CONFIG.get("abstract_tasks_wait_timeout", 300)
            logger.info(f"ç­‰å¾… {len(abstract_tasks)} ä¸ªabstractç”Ÿæˆä»»åŠ¡å®Œæˆï¼Œè¶…æ—¶: {abstract_wait_timeout}ç§’")
            try:
                await asyncio.wait_for(
                    asyncio.gather(*abstract_tasks, return_exceptions=True),
                    timeout=abstract_wait_timeout
                )
                logger.info("æ‰€æœ‰abstractç”Ÿæˆä»»åŠ¡å·²å®Œæˆ")
            except asyncio.TimeoutError:
                logger.warning(f"Abstractç”Ÿæˆä»»åŠ¡ç­‰å¾…è¶…æ—¶ ({abstract_wait_timeout}ç§’)ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡")
                # å–æ¶ˆæœªå®Œæˆçš„abstractä»»åŠ¡
                for task in abstract_tasks:
                    if not task.done():
                        task.cancel()
            except Exception as e:
                logger.error(f"ç­‰å¾…abstractä»»åŠ¡æ—¶å‡ºç°å¼‚å¸¸: {e}")

    results = []
    while not output_queue.empty():
        results.append(await output_queue.get())

    return results

def _clean_html_content(html_content: str, url: str) -> Dict[str, Any]:
    """
    Clean HTML content using the integrated HTML cleaner

    Args:
        html_content (str): Raw HTML content
        url (str): URL for context

    Returns:
        Dict[str, Any]: Cleaned content with keys: text, language, cleaned, error
    """
    # åŸºæœ¬æ¸…ç†å‡½æ•°
    def basic_clean(content):
        import re
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾åŠå…¶å†…å®¹
        content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content).strip()
        return content

    # å°è¯•ä½¿ç”¨é«˜çº§HTMLæ¸…ç†å™¨
    if HTML_CLEANER_AVAILABLE and _html_extractor:
        try:
            extracted = _html_extractor.extract(html_content)

            if extracted and extracted.get("text"):
                return {
                    "text": extracted["text"],
                    "language": extracted.get("language", "UNKNOWN"),
                    "cleaned": True,
                    "error": False,
                    "method": "advanced"
                }
        except Exception as e:
            logger.debug(f"Advanced HTML cleaning failed for URL={url}: {e}")
            # ç»§ç»­åˆ°åŸºæœ¬æ¸…ç†

    # ä½¿ç”¨åŸºæœ¬æ¸…ç†ä½œä¸ºé»˜è®¤æˆ–å›é€€æ–¹æ¡ˆ
    try:
        basic_text = basic_clean(html_content)
        return {
            "text": basic_text,
            "language": "UNKNOWN",
            "cleaned": True,
            "error": False,
            "method": "basic"
        }
    except Exception as e:
        logger.error(f"Basic HTML cleaning failed for URL={url}: {e}")
        return {
            "text": html_content,  # è¿”å›åŸå§‹å†…å®¹ä½œä¸ºæœ€åçš„å›é€€
            "language": "UNKNOWN",
            "cleaned": False,
            "error": True,
            "method": "none",
            "error_message": str(e)
        }

async def _crawl_single_url(url: str, topic: str) -> Dict[str, Any]:
    """
    Crawl a single URL and return the result with integrated HTML cleaning.

    Args:
        url (str): URL to crawl
        topic (str): The topic for context

    Returns:
        Dict[str, Any]: Crawl result with keys: topic, url, raw_content, cleaned_content,
                       title, language, error, cleaning_info
    """
    try:
        if not CRAWL4AI_AVAILABLE:
            raise ImportError("crawl4ai is not available")

        # crawl4aiéœ€è¦æ¯«ç§’å•ä½çš„è¶…æ—¶æ—¶é—´ï¼Œä¼˜å…ˆä½¿ç”¨page_timeouté…ç½®
        page_timeout_seconds = SERVER_CONFIG.get("page_timeout", SERVER_CONFIG.get("single_url_crawl_timeout", 60))
        page_timeout_ms = page_timeout_seconds * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

        crawler_run_config = CrawlerRunConfig(
            page_timeout=page_timeout_ms,
            cache_mode=CacheMode.BYPASS
        )

        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=crawler_run_config)

            # æ£€æŸ¥çˆ¬å–ç»“æœæ˜¯å¦æˆåŠŸ
            if not result:
                raise Exception("Crawler returned None result")

            # è·å–åŸå§‹å†…å®¹å’ŒHTMLå†…å®¹
            raw_content = None
            html_content = None
            title = ""

            # å°è¯•è·å–æ ‡é¢˜
            if hasattr(result, 'metadata') and result.metadata:
                title = result.metadata.get('title', '')

            # æ£€æŸ¥markdown_v2æ˜¯å¦å­˜åœ¨
            if not hasattr(result, 'markdown_v2') or result.markdown_v2 is None:
                logger.warning(f"No markdown_v2 found for URL={url}, trying alternative content")
                # å°è¯•ä½¿ç”¨å…¶ä»–å†…å®¹å­—æ®µ
                if hasattr(result, 'markdown') and result.markdown:
                    raw_content = result.markdown
                elif hasattr(result, 'cleaned_html') and result.cleaned_html:
                    raw_content = result.cleaned_html
                    html_content = result.cleaned_html
                elif hasattr(result, 'html') and result.html:
                    raw_content = result.html
                    html_content = result.html
                else:
                    raise Exception("No usable content found in crawl result")
            else:
                # æ£€æŸ¥raw_markdownæ˜¯å¦å­˜åœ¨
                if hasattr(result.markdown_v2, 'raw_markdown') and result.markdown_v2.raw_markdown:
                    raw_content = result.markdown_v2.raw_markdown
                else:
                    logger.warning(f"No raw_markdown found for URL={url}, trying alternative markdown content")
                    # å°è¯•ä½¿ç”¨markdown_v2çš„å…¶ä»–å­—æ®µ
                    if hasattr(result.markdown_v2, 'markdown') and result.markdown_v2.markdown:
                        raw_content = result.markdown_v2.markdown
                    else:
                        raise Exception("No usable markdown content found")

                # å°è¯•è·å–HTMLå†…å®¹ç”¨äºæ¸…ç†
                if hasattr(result, 'html') and result.html:
                    html_content = result.html

            # è¿›è¡ŒHTMLå†…å®¹æ¸…ç†
            cleaned_info = {"method": "none", "error": False}
            cleaned_content = raw_content
            detected_language = "UNKNOWN"

            if html_content and len(html_content.strip()) > 0:
                cleaning_result = _clean_html_content(html_content, url)
                if not cleaning_result.get("error", True) and cleaning_result.get("text"):
                    cleaned_content = cleaning_result["text"]
                    detected_language = cleaning_result.get("language", "UNKNOWN")
                    cleaned_info = {
                        "method": cleaning_result.get("method", "unknown"),
                        "error": cleaning_result.get("error", False),
                        "error_message": cleaning_result.get("error_message", "")
                    }
                    logger.info(f"Content cleaned using {cleaning_result.get('method')} method for URL={url}")

            logger.info(f"Content length={len(raw_content)} (raw), {len(cleaned_content)} (cleaned) for URL={url}")

            return {
                "topic": topic,
                "url": url,
                "raw_content": raw_content,  # ä¿æŒåŸæœ‰å­—æ®µå
                "cleaned_content": cleaned_content,  # æ–°å¢æ¸…ç†åçš„å†…å®¹
                "title": title,
                "language": detected_language,
                "error": False,
                "cleaning_info": cleaned_info
            }
    except Exception as e:
        logger.error(f"Crawling failed for URL={url}: {e}")
        return {
            "topic": topic,
            "url": url,
            "raw_content": f"Error: Crawling failed({e})",
            "cleaned_content": "",
            "title": "",
            "language": "UNKNOWN",
            "error": True,
            "cleaning_info": {"method": "none", "error": True, "error_message": str(e)}
        }

async def _process_filter_and_titles_stage(crawl_results: List[Dict[str, Any]], request_wrapper) -> List[Dict[str, Any]]:
    import asyncio

    MAX_CONCURRENT_PROCESSES = 10
    input_queue = asyncio.Queue()
    output_queue = asyncio.Queue()

    for data in crawl_results:
        if not data.get("error", False):
            await input_queue.put(data)

    async def processor():
        while True:
            try:
                data = input_queue.get_nowait()
                try:
                    result = await _process_filter_and_title_single(data, request_wrapper)
                    await output_queue.put(result)
                finally:
                    input_queue.task_done()
            except asyncio.QueueEmpty:
                break

    processors = [asyncio.create_task(processor()) for _ in range(MAX_CONCURRENT_PROCESSES)]

    await input_queue.join()

    for processor_task in processors:
        processor_task.cancel()

    results = []
    while not output_queue.empty():
        results.append(await output_queue.get())

    return results

async def _process_filter_and_title_single(data: Dict[str, Any], request_wrapper) -> Dict[str, Any]:
    import re

    try:
        prompt_template_response = await read_resource("llm://search/prompts")
        prompt_template = prompt_template_response[0].text
        prompts = json.loads(prompt_template)

        prompt = prompts["page_refine"].format(
            topic=data["topic"], raw_content=data["raw_content"]
        )
        res = request_wrapper.completion(prompt)

        # å°è¯•æå–æ ‡é¢˜å’Œå†…å®¹
        title = re.search(r"<TITLE>(.*?)</TITLE>", res, re.DOTALL)
        content = re.search(r"<CONTENT>(.*?)</CONTENT>", res, re.DOTALL)

        if not title or not content:
            # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œå°è¯•ä»åŸå§‹å†…å®¹ä¸­æå–
            logger.warning(f"Invalid response format for URL={data.get('url', 'unknown')}, trying fallback extraction")

            # å°è¯•ä»åŸå§‹å†…å®¹ä¸­æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œæˆ–å‰å‡ è¡Œï¼‰
            raw_content = data.get("raw_content", "")
            lines = raw_content.split('\n')

            # å¯»æ‰¾å¯èƒ½çš„æ ‡é¢˜
            potential_title = ""
            for line in lines[:10]:  # æ£€æŸ¥å‰10è¡Œ
                line = line.strip()
                if line and len(line) < 200:  # æ ‡é¢˜é€šå¸¸è¾ƒçŸ­
                    potential_title = line
                    break

            if not potential_title:
                potential_title = f"Content from {data.get('url', 'unknown')}"

            # ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºè¿‡æ»¤åçš„å†…å®¹ï¼ˆç®€å•æ¸…ç†ï¼‰
            filtered_content = raw_content
            # ç®€å•æ¸…ç†ï¼šç§»é™¤è¿‡å¤šçš„ç©ºè¡Œå’Œç‰¹æ®Šå­—ç¬¦
            filtered_content = re.sub(r'\n\s*\n\s*\n', '\n\n', filtered_content)
            filtered_content = re.sub(r'[^\w\s\u4e00-\u9fff.,;:!?()[\]{}"\'-]', ' ', filtered_content)

            data["title"] = potential_title[:200]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
            data["content"] = filtered_content[:10000]  # é™åˆ¶å†…å®¹é•¿åº¦
            data["filter_error"] = False

            logger.info(f"Used fallback extraction for URL={data.get('url', 'unknown')}")
        else:
            data["title"] = title.group(1).strip()
            data["content"] = content.group(1).strip()
            data["filter_error"] = False

        return data

    except Exception as e:
        logger.error(f"Content filtering failed for URL={data.get('url', 'unknown')}: {e}")
        data["title"] = f"Error processing: {data.get('url', 'unknown')}"
        data["content"] = f"Error: Content filtering failed({e})"
        data["filter_error"] = True
        return data

async def _process_similarity_scores_stage(filtered_results: List[Dict[str, Any]], request_wrapper) -> List[Dict[str, Any]]:

    import asyncio

    MAX_CONCURRENT_PROCESSES = 10
    input_queue = asyncio.Queue()
    output_queue = asyncio.Queue()

    for data in filtered_results:
        if not data.get("filter_error", False):
            await input_queue.put(data)

    async def scorer():
        while True:
            try:
                data = input_queue.get_nowait()
                try:
                    result = await _process_similarity_score_single(data, request_wrapper)
                    await output_queue.put(result)
                finally:
                    input_queue.task_done()
            except asyncio.QueueEmpty:
                break

    scorers = [asyncio.create_task(scorer()) for _ in range(MAX_CONCURRENT_PROCESSES)]

    await input_queue.join()

    for scorer_task in scorers:
        scorer_task.cancel()

    results = []
    while not output_queue.empty():
        results.append(await output_queue.get())

    return results

async def _process_similarity_score_single(data: Dict[str, Any], request_wrapper) -> Dict[str, Any]:
    import re

    try:
        prompt_template_response = await read_resource("llm://search/prompts")
        prompt_template = prompt_template_response[0].text
        prompts = json.loads(prompt_template)
        
        prompt = prompts["similarity_scoring"].format(
            topic=data["topic"], content=data["content"]
        )
        res = request_wrapper.completion(prompt)
        score_match = re.search(r"<SCORE>(\d+)</SCORE>", res)

        if score_match:
            data["similarity_score"] = int(score_match.group(1))
        else:
            data["similarity_score"] = 50
            logger.warning(f"No score found in response for URL={data.get('url', 'unknown')}, using default score 50")

        data["score_error"] = False
        return data

    except Exception as e:
        logger.error(f"Similarity scoring failed for URL={data.get('url', 'unknown')}: {e}")
        data["similarity_score"] = 0
        data["score_error"] = True
        return data

def _load_queries_from_file(query_file_path: str = None, topic: str = "") -> List[str]:
    """
    ä»æ–‡ä»¶ä¸­è¯»å–æŸ¥è¯¢åˆ—è¡¨

    Args:
        query_file_path: æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        topic: æœç´¢ä¸»é¢˜ï¼Œç”¨äºæŸ¥æ‰¾ç›¸å…³æ–‡ä»¶

    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    try:
        import os
        import glob

        # å¦‚æœæ²¡æœ‰æä¾›æ–‡ä»¶è·¯å¾„ï¼Œä»query_cacheç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        if query_file_path is None:
            cache_dir = SERVER_CONFIG.get("query_cache_dir", "query_cache")
            cache_dir = os.path.join(os.path.dirname(__file__), '..', '..', cache_dir)
            if not os.path.exists(cache_dir):
                logger.warning(f"Query cache directory not found: {cache_dir}")
                return []

            # æŸ¥æ‰¾æ‰€æœ‰æŸ¥è¯¢æ–‡ä»¶
            pattern = os.path.join(cache_dir, "queries_*.json")
            query_files = glob.glob(pattern)

            if not query_files:
                logger.warning(f"No query files found in: {cache_dir}")
                return []

            # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
            query_file_path = max(query_files, key=os.path.getmtime)
            logger.info(f"Using latest query file: {query_file_path}")

        # è¯»å–æ–‡ä»¶
        if not os.path.exists(query_file_path):
            logger.error(f"Query file not found: {query_file_path}")
            return []

        with open(query_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        queries = data.get("queries", [])
        logger.info(f"âœ… Loaded {len(queries)} queries from: {query_file_path}")
        return queries

    except Exception as e:
        logger.error(f"âŒ Failed to load queries from file: {e}")
        return []

def _load_urls_from_file(url_file_path: str = None, topic: str = "") -> List[str]:
    """
    ä»æ–‡ä»¶ä¸­è¯»å–URLåˆ—è¡¨

    Args:
        url_file_path: URLæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        topic: æœç´¢ä¸»é¢˜ï¼Œç”¨äºæŸ¥æ‰¾ç›¸å…³æ–‡ä»¶

    Returns:
        URLåˆ—è¡¨
    """
    try:
        import os
        import glob

        # å¦‚æœæ²¡æœ‰æä¾›æ–‡ä»¶è·¯å¾„ï¼Œä»url_cacheç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        if url_file_path is None:
            cache_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'url_cache')
            if not os.path.exists(cache_dir):
                logger.warning(f"URL cache directory not found: {cache_dir}")
                return []

            # æŸ¥æ‰¾æ‰€æœ‰URLæ–‡ä»¶
            pattern = os.path.join(cache_dir, "urls_*.json")
            url_files = glob.glob(pattern)

            if not url_files:
                logger.warning(f"No URL files found in: {cache_dir}")
                return []

            # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
            url_file_path = max(url_files, key=os.path.getmtime)
            logger.info(f"Using latest URL file: {url_file_path}")

        # è¯»å–æ–‡ä»¶
        if not os.path.exists(url_file_path):
            logger.error(f"URL file not found: {url_file_path}")
            return []

        with open(url_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        urls = data.get("urls", [])
        logger.info(f"âœ… Loaded {len(urls)} URLs from: {url_file_path}")
        return urls

    except Exception as e:
        logger.error(f"âŒ Failed to load URLs from file: {e}")
        return []

def _save_queries_to_file(queries: List[str], topic: str, description: str = "") -> str:
    """
    å°†æŸ¥è¯¢åˆ—è¡¨ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        topic: æœç´¢ä¸»é¢˜
        description: ä¸»é¢˜æè¿°

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¿å­˜å¤±è´¥åˆ™è¿”å›None
    """
    try:
        import time
        import os

        # åˆ›å»ºä¿å­˜ç›®å½•
        cache_dir = SERVER_CONFIG.get("query_cache_dir", "query_cache")
        save_dir = os.path.join(os.path.dirname(__file__), '..', '..', cache_dir)
        os.makedirs(save_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œä¸»é¢˜ï¼‰
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_topic = safe_topic.replace(' ', '_')[:30]  # é™åˆ¶é•¿åº¦
        filename = f"queries_{safe_topic}_{timestamp}.json"
        filepath = os.path.join(save_dir, filename)

        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            "topic": topic,
            "description": description,
            "queries": queries,
            "query_count": len(queries),
            "timestamp": timestamp,
            "metadata": {
                "generated_by": "generate_search_queries_tool",
                "version": "1.0",
                "description": f"Search queries for topic: {topic}"
            }
        }

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… Queries saved to: {filepath}")
        return filepath

    except Exception as e:
        logger.error(f"âŒ Failed to save queries to file: {e}")
        return None

def _save_urls_to_file(urls: List[str], topic: str, queries: List[str]) -> str:
    """
    å°†URLåˆ—è¡¨ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶

    Args:
        urls: URLåˆ—è¡¨
        topic: æœç´¢ä¸»é¢˜
        queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¿å­˜å¤±è´¥åˆ™è¿”å›None
    """
    try:
        import time
        import os

        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'url_cache')
        os.makedirs(save_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œä¸»é¢˜ï¼‰
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_topic = safe_topic.replace(' ', '_')[:30]  # é™åˆ¶é•¿åº¦
        filename = f"urls_{safe_topic}_{timestamp}.json"
        filepath = os.path.join(save_dir, filename)

        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            "topic": topic,
            "queries": queries,
            "urls": urls,
            "url_count": len(urls),
            "timestamp": timestamp,
            "metadata": {
                "generated_by": "web_search_tool",
                "version": "1.0",
                "description": f"Web search URLs for topic: {topic}"
            }
        }

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… URLs saved to: {filepath}")
        return filepath

    except Exception as e:
        logger.error(f"âŒ Failed to save URLs to file: {e}")
        return None

def _process_and_sort_results(scored_results: List[Dict[str, Any]], top_n: int,
                             similarity_threshold: float, min_length: int, max_length: int) -> List[Dict[str, Any]]:

    filtered_results = []
    for result in scored_results:
        content_length = len(result.get("content", ""))
        score_error = result.get("score_error", False)
        similarity_score = result.get("similarity_score", 0)

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        logger.info(f"Processing result: URL={result.get('url', 'Unknown')}")
        logger.info(f"  - content_length: {content_length}")
        logger.info(f"  - score_error: {score_error}")
        logger.info(f"  - similarity_score: {similarity_score}")
        logger.info(f"  - similarity_threshold: {similarity_threshold}")
        logger.info(f"  - max_length: {max_length}")

        # æ”¾å®½é•¿åº¦é™åˆ¶ï¼Œåªè¦æœ‰å†…å®¹å°±ä¿ç•™
        if (not score_error and
            similarity_score >= similarity_threshold and
            content_length > 0 and content_length <= max_length):
            filtered_results.append(result)
            logger.info(f"  âœ… Result PASSED all filters")
        else:
            logger.info(f"  âŒ Result FAILED filters:")
            if score_error:
                logger.info(f"    - score_error: {score_error}")
            if similarity_score < similarity_threshold:
                logger.info(f"    - similarity_score ({similarity_score}) < threshold ({similarity_threshold})")
            if content_length <= 0:
                logger.info(f"    - content_length ({content_length}) <= 0")
            if content_length > max_length:
                logger.info(f"    - content_length ({content_length}) > max_length ({max_length})")

    filtered_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)

    final_results = filtered_results[:top_n]

    # ç”Ÿæˆå…¼å®¹ LLMxMapReduce_V2 å’Œ Survey æ ¼å¼çš„ç»“æœ
    formatted_results = []
    for i, result in enumerate(final_results):
        content = result.get("content", "")
        title = result.get("title", "")

        # ç”Ÿæˆå…¼å®¹æ ¼å¼çš„è®ºæ–‡æ•°æ®
        paper_data = {
            # LLMxMapReduce_V2 æ ¸å¿ƒå­—æ®µ
            "title": title,
            "url": result.get("url", ""),
            "txt": content,  # content â†’ txt
            "similarity": result.get("similarity_score", 0),  # similarity_score â†’ similarity

            # Survey å…¼å®¹å­—æ®µ
            "bibkey": proc_title_to_str(title),
            "abstract": extract_abstract(content, 500),
            "txt_token": estimate_tokens(content),
            "txt_length": len(content),

            # å…ƒæ•°æ®å­—æ®µ
            "source_type": "web_crawl",
            "crawl_timestamp": time.time(),
            "processing_stage": "crawl_complete"
        }
        formatted_results.append(paper_data)

    return formatted_results

async def main():
    logger.info("Starting LLM Search MCP Server...")
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await app.run(read_stream, write_stream, app.create_initialization_options())

if __name__ == "__main__":
    asyncio.run(main())
