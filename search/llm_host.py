#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Host Layer

è¿™æ˜¯MCPæ¶æ„ä¸­çš„Hostå±‚ï¼Œè´Ÿè´£ï¼š
1. æä¾›é«˜çº§ä¸šåŠ¡æ¥å£
2. ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å·¥å…·é€‰æ‹©å’Œä»»åŠ¡å¤„ç†
3. è°ƒç”¨Clientå±‚ä¸MCPæœåŠ¡å™¨é€šä¿¡
4. ä¸æš´éœ²åº•å±‚MCPå·¥å…·å®ç°ç»†èŠ‚
"""

import asyncio
import json
import logging
import os
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

from .llm_search_mcp_client import create_mcp_client

# å¯¼å…¥RequestWrapperç”¨äºLLMè°ƒç”¨
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from request.wrapper import RequestWrapper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LLMHost:
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path
        self.mcp_client = None
        self._connected = False
        self.env_config = self._load_environment_config()
        self.available_tools = []
        self.operation_history = []
        self.conversation_history = []
        # ä»é…ç½®ä¸­è¯»å–max_rounds
        analyse_settings = self.env_config.get("analyse_settings", {})
        self.max_rounds = analyse_settings.get("llm_host_max_rounds", 10)
        
        # åˆå§‹åŒ–LLMç»„ä»¶
        self._init_llm_components()
        
    def _load_environment_config(self):
        """åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶"""
        try:
            config_paths = [
                "new/config/unified_config.json",
                "config/unified_config.json",
                os.path.join(os.path.dirname(__file__), "..", "..", "config", "unified_config.json")
            ]

            for config_path in config_paths:
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        return json.load(f)

            logger.error("Environment config file not found! Please ensure config/unified_config.json exists")
            raise FileNotFoundError("Configuration file config/unified_config.json is required but not found")
        except Exception as e:
            logger.error(f"Failed to load environment config: {e}")
            return {}
    
    def _init_llm_components(self):
        """åˆå§‹åŒ–LLMç»„ä»¶"""
        try:
            models = self.env_config.get("models", {})
            # ä¼˜å…ˆä½¿ç”¨host_llm_modelï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨default_model
            model = models.get("host_llm_model") or models.get("default_model")
            infer_type = models.get("default_infer_type")

            if not model:
                raise ValueError("No model specified in configuration. Please set 'host_llm_model' or 'default_model' in config/unified_config.json")
            if not infer_type:
                raise ValueError("No infer_type specified in configuration. Please set 'default_infer_type' in config/unified_config.json")
            
            self.llm_wrapper = RequestWrapper(
                model=model,
                infer_type=infer_type
            )
            logger.info(f"LLM wrapper initialized with model: {model}")
        except Exception as e:
            logger.error(f"Failed to initialize LLM wrapper: {e}")
            raise
    
    async def connect(self):
        """è¿æ¥åˆ°MCPæœåŠ¡å™¨"""
        if self._connected:
            return
            
        try:
            # å‡†å¤‡MCPæœåŠ¡å™¨é…ç½®
            server_config = self._prepare_server_config()
            
            # åˆ›å»ºMCPå®¢æˆ·ç«¯
            self.mcp_client = await create_mcp_client(server_config)
            self._connected = True
            
            # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
            self.available_tools = await self.mcp_client.list_tools()
            
            logger.info("Successfully connected to LLM MCP Server")
            logger.info(f"Available tools: {[tool['name'] for tool in self.available_tools]}")
            
        except Exception as e:
            logger.error(f"Failed to connect to MCP server: {e}")
            raise
    
    def _prepare_server_config(self) -> Dict[str, Any]:
        """å‡†å¤‡MCPæœåŠ¡å™¨é…ç½® - å¤ç”¨LLMSearchHostçš„é€»è¾‘"""
        # åªè®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼Œé¿å…ç³»ç»Ÿç¯å¢ƒå˜é‡å†²çª
        env_vars = {
            "PYTHONPATH": ".",
            "PATH": os.environ.get("PATH", ""),
            "SYSTEMROOT": os.environ.get("SYSTEMROOT", ""),
            "TEMP": os.environ.get("TEMP", ""),
            "TMP": os.environ.get("TMP", ""),
        }

        # ä»é…ç½®æ–‡ä»¶è®¾ç½®APIå¯†é’¥
        api_keys = self.env_config.get("api_keys", {})

        openai_config = api_keys.get("openai", {})
        if openai_config.get("api_key"):
            env_vars["OPENAI_API_KEY"] = openai_config["api_key"]
            logger.info("âœ… OPENAI_API_KEY å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨")
        if openai_config.get("base_url"):
            env_vars["OPENAI_BASE_URL"] = openai_config["base_url"]
            env_vars["OPENAI_API_BASE"] = openai_config["base_url"]
            logger.info("âœ… OPENAI_BASE_URL å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨")

        search_engines = api_keys.get("search_engines", {})
        if search_engines.get("serpapi_key"):
            env_vars["SERPAPI_KEY"] = search_engines["serpapi_key"]
            env_vars["SERP_API_KEY"] = search_engines["serpapi_key"]
            logger.info(f"âœ… SERPAPI_KEY å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨: {search_engines['serpapi_key'][:10]}...")
        if search_engines.get("bing_subscription_key"):
            env_vars["BING_SEARCH_V7_SUBSCRIPTION_KEY"] = search_engines["bing_subscription_key"]
            logger.info("âœ… BING_SEARCH_V7_SUBSCRIPTION_KEY å·²ä¼ é€’ç»™MCPæœåŠ¡å™¨")

        # æ·»åŠ Google API Keyï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if openai_config.get("api_key"):
            env_vars["GOOGLE_API_KEY"] = openai_config["api_key"]

        # è·å–é¡¹ç›®æ ¹ç›®å½•
        current_file = Path(__file__).resolve()
        project_root = current_file.parent.parent.parent

        # ä½¿ç”¨æ¨¡å—å¯¼å…¥æ–¹å¼å¯åŠ¨ï¼Œä¸é…ç½®æ–‡ä»¶ä¿æŒä¸€è‡´
        return {
            "command": "python",
            "args": ["-m", "src.search.llm_search_mcp_server"],
            "env": env_vars,
            "cwd": str(project_root)
        }
    
    async def _cleanup_after_task_completion(self):
        """
        Clean up MCP processes after task completion following MCP best practices
        """
        logger.info("Starting post-task cleanup process")

        try:
            # Step 1: Gracefully disconnect from MCP server
            if self.mcp_client and self._connected:
                logger.info("Gracefully disconnecting MCP client after task completion...")
                try:
                    await self.mcp_client.disconnect()
                    logger.info("MCP client disconnected successfully")
                except Exception as disconnect_error:
                    # Log the error but don't let it stop the cleanup process
                    logger.warning(f"MCP client disconnect had issues (continuing cleanup): {disconnect_error}")
                finally:
                    # Always clear the client reference
                    self.mcp_client = None

            # Step 2: Reset connection state
            self._connected = False

            # Step 3: Clear operation history for next task
            self.operation_history = []
            self.conversation_history = []

            logger.info("Post-task cleanup completed successfully")

        except Exception as e:
            logger.error(f"Error during post-task cleanup: {e}")
            # Force cleanup even if there were errors
            self.mcp_client = None
            self._connected = False
            self.operation_history = []
            self.conversation_history = []

    async def disconnect(self):
        """
        Safely disconnect from MCP server with improved error handling
        """
        logger.info("Starting LLMHost disconnect process")

        try:
            if self.mcp_client and self._connected:
                logger.info("Disconnecting MCP client...")
                try:
                    await self.mcp_client.disconnect()
                    logger.info("MCP client disconnected successfully")
                except Exception as disconnect_error:
                    logger.warning(f"MCP client disconnect had issues (this is often normal): {disconnect_error}")
                    # Continue with cleanup even if disconnect had issues
                finally:
                    self.mcp_client = None

            self._connected = False
            logger.info("LLMHost disconnected successfully")

        except Exception as e:
            logger.error(f"Error during LLMHost disconnect: {e}")
            # Force cleanup even if there were errors
            self.mcp_client = None
            self._connected = False
    
    async def process_task(self, task_description: str, context: str = "") -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·ä»»åŠ¡ - ä¸»è¦ä¸šåŠ¡æ¥å£
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            context: ä»»åŠ¡ä¸Šä¸‹æ–‡
            
        Returns:
            ä»»åŠ¡å¤„ç†ç»“æœ
        """
        if not self._connected:
            await self.connect()
            
        try:
            logger.info(f"Starting task processing: '{task_description}'")
            
            # æ¸…ç©ºæ“ä½œå†å²
            self.operation_history = []
            
            # å¼€å§‹LLMå†³ç­–å¾ªç¯
            result = await self._llm_decision_loop(task_description, context)
            
            logger.info(f"Task processing completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"Error in task processing: {e}")
            raise
    
    async def _llm_decision_loop(self, task_description: str, context: str) -> Dict[str, Any]:
        """LLMå†³ç­–å¾ªç¯ - æ ¸å¿ƒé€»è¾‘"""
        for round_num in range(1, self.max_rounds + 1):
            logger.info(f"=== Decision Round {round_num}/{self.max_rounds} ===")
            
            try:
                # è°ƒç”¨LLMè¿›è¡Œå†³ç­–
                decision = await self._call_llm_for_decision(task_description, context, round_num)

                if not decision:
                    logger.warning(f"No decision received in round {round_num}, ending task")
                    return {
                        "status": "completed",
                        "result": "Task ended - No decision from LLM",
                        "rounds_used": round_num,
                        "operation_history": self.operation_history
                    }

                action = decision.get("action")

                # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœactionæ˜¯å·¥å…·åç§°ï¼Œè½¬æ¢ä¸ºcall_toolæ ¼å¼
                if action in [tool["name"] for tool in self.available_tools]:
                    logger.info(f"Converting tool name action '{action}' to call_tool format")
                    decision = {
                        "action": "call_tool",
                        "tool_name": action,
                        "arguments": decision.get("arguments", {})
                    }
                    action = "call_tool"

                # æ£€æŸ¥æ˜¯å¦æ˜¯Noneå·¥å…·è°ƒç”¨ï¼ˆé€€å‡ºä¿¡å·ï¼‰
                if action == "call_tool" and decision.get("tool_name") is None:
                    logger.info("LLM indicated None tool, ending task")
                    # Task completed - disconnect MCP processes
                    await self._cleanup_after_task_completion()

                    return {
                        "status": "completed",
                        "result": "Task completed - LLM indicated no further tools needed",
                        "rounds_used": round_num,
                        "operation_history": self.operation_history
                    }
                
                if action == "complete":
                    logger.info("LLM decided to complete the task")
                    result = decision.get("result", "Task completed")

                    # Task completed - disconnect MCP processes
                    await self._cleanup_after_task_completion()

                    return {
                        "status": "completed",
                        "result": result,
                        "rounds_used": round_num,
                        "operation_history": self.operation_history
                    }
                
                elif action == "call_tool":
                    tool_name = decision.get("tool_name")
                    arguments = decision.get("arguments", {})
                    
                    if not tool_name:
                        logger.warning("Tool name not specified in decision")
                        continue
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_result = await self._execute_tool_call(tool_name, arguments)
                    
                    # è®°å½•æ“ä½œå†å²
                    self.operation_history.append({
                        "round": round_num,
                        "action": "call_tool",
                        "tool_name": tool_name,
                        "arguments": arguments,
                        "result": tool_result
                    })
                
                elif action == "request_info":
                    message = decision.get("message", "Need more information")
                    logger.info(f"LLM requests info: {message}")
                    
                    # è®°å½•ä¿¡æ¯è¯·æ±‚
                    self.operation_history.append({
                        "round": round_num,
                        "action": "request_info",
                        "message": message
                    })
                
                else:
                    logger.warning(f"Unknown action: {action}")
                    continue
                    
            except Exception as e:
                logger.error(f"Error in decision round {round_num}: {e}")
                self.operation_history.append({
                    "round": round_num,
                    "action": "error",
                    "error": str(e)
                })
        
        # è¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶
        logger.warning(f"Reached maximum rounds ({self.max_rounds})")

        # Task reached max rounds - disconnect MCP processes
        await self._cleanup_after_task_completion()

        return {
            "status": "max_rounds_reached",
            "result": "Task processing reached maximum rounds limit",
            "rounds_used": self.max_rounds,
            "operation_history": self.operation_history
        }

    async def _call_llm_for_decision(self, task_description: str, context: str, round_num: int) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨LLMè¿›è¡Œå†³ç­–"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt()

            # æ„å»ºå½“å‰çŠ¶æ€æè¿°
            current_state = self._build_current_state(task_description, context, round_num)

            # ç»´æŠ¤å¯¹è¯å†å² - åªåœ¨ç¬¬ä¸€è½®æ·»åŠ åˆå§‹æ¶ˆæ¯
            if round_num == 1:
                self.conversation_history = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": current_state}
                ]
            else:
                # åç»­è½®æ¬¡åªæ·»åŠ å½“å‰çŠ¶æ€æ›´æ–°
                self.conversation_history.append({"role": "user", "content": current_state})

            # è°ƒç”¨LLM
            response = await self.llm_wrapper.async_request(self.conversation_history)

            # å°†LLMå“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
            self.conversation_history.append({"role": "assistant", "content": response})

            # è§£æå“åº”
            decision = self._parse_llm_response(response)

            logger.info(f"LLM decision: {decision}")
            return decision

        except Exception as e:
            logger.error(f"Error calling LLM for decision: {e}")
            return None

    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        tools_info = []
        for tool in self.available_tools:
            name = tool.get("name", "")
            description = tool.get("description", "")

            # æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯
            input_schema = tool.get("inputSchema", {})
            properties = input_schema.get("properties", {})
            required = input_schema.get("required", [])

            params_info = []
            for prop, prop_info in properties.items():
                prop_type = prop_info.get("type", "string")
                prop_desc = prop_info.get("description", "")
                is_required = prop in required
                req_marker = " (å¿…éœ€)" if is_required else " (å¯é€‰)"
                params_info.append(f"  - {prop}: {prop_type}{req_marker} - {prop_desc}")

            params_str = "\n".join(params_info) if params_info else "  æ— å‚æ•°"
            tools_info.append(f"- {name}: {description}\n{params_str}")

        tools_description = "\n\n".join(tools_info)

        # ä»é…ç½®è¯»å–LLMHostç³»ç»Ÿæç¤ºè¯
        prompts = self.env_config.get("prompts", {})
        system_prompt_template = prompts.get("llm_host_system",
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡å¤„ç†åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§å·¥å…·æ¥å®Œæˆç”¨æˆ·çš„ä»»åŠ¡ã€‚\n\nå¯ç”¨å·¥å…·ï¼š\n{tools_description}")

        return system_prompt_template.format(tools_description=tools_description)

    def _build_current_state(self, task_description: str, context: str, round_num: int) -> str:
        """æ„å»ºå½“å‰çŠ¶æ€æè¿°"""
        state_parts = [
            f"ä»»åŠ¡æè¿°ï¼š{task_description}",
            f"ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼š{context}" if context else "",
            f"å½“å‰è½®æ¬¡ï¼š{round_num}/{self.max_rounds}"
        ]

        if self.operation_history:
            state_parts.append("\næ“ä½œå†å²ï¼š")
            for i, op in enumerate(self.operation_history, 1):
                round_info = op.get("round", i)
                action = op.get("action", "unknown")

                if action == "call_tool":
                    tool_name = op.get("tool_name", "unknown")
                    result = op.get("result", {})
                    # ç®€åŒ–ç»“æœæ˜¾ç¤ºï¼Œé¿å…è¿‡é•¿
                    result_summary = str(result)[:200] + "..." if len(str(result)) > 200 else str(result)
                    state_parts.append(f"  {round_info}. è°ƒç”¨å·¥å…· {tool_name}ï¼Œç»“æœï¼š{result_summary}")
                elif action == "request_info":
                    message = op.get("message", "")
                    state_parts.append(f"  {round_info}. è¯·æ±‚ä¿¡æ¯ï¼š{message}")
                elif action == "error":
                    error = op.get("error", "")
                    state_parts.append(f"  {round_info}. å‘ç”Ÿé”™è¯¯ï¼š{error}")
        else:
            state_parts.append("\næ“ä½œå†å²ï¼šæ— ")

        state_parts.append(f"\nè¯·åˆ†æå½“å‰çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š")

        return "\n".join(filter(None, state_parts))

    def _parse_llm_response(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æLLMçš„JSONå“åº”"""
        try:
            # æ¸…ç†å“åº”å†…å®¹
            cleaned_response = response.strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯Noneæˆ–ç©ºå“åº”
            if not cleaned_response or cleaned_response.lower() in ['none', 'null', '']:
                logger.info("LLM returned None/empty response, ending task")
                return {"action": "complete", "result": "Task completed - LLM indicated no further action needed"}

            # å°è¯•ç›´æ¥è§£æJSON
            if cleaned_response.startswith('{') and cleaned_response.endswith('}'):
                return json.loads(cleaned_response)

            # å°è¯•ä»ä»£ç å—ä¸­æå–JSON
            import re
            json_pattern = r'```(?:json)?\s*(.*?)\s*```'
            match = re.search(json_pattern, cleaned_response, re.DOTALL)
            if match:
                json_content = match.group(1).strip()
                if json_content:
                    return json.loads(json_content)

            # å°è¯•æå–å¤§æ‹¬å·å†…å®¹
            brace_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            match = re.search(brace_pattern, cleaned_response, re.DOTALL)
            if match:
                return json.loads(match.group(0))

            # å¦‚æœæ— æ³•è§£æï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯å¹¶è¿”å›å®ŒæˆçŠ¶æ€
            logger.warning(f"Could not parse LLM response as JSON: {cleaned_response}")
            return {"action": "complete", "result": f"Task completed - Unable to parse LLM response: {cleaned_response[:100]}..."}

        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON response: {e}")
            logger.warning(f"Response content: {response}")
            return {"action": "complete", "result": f"Task completed - JSON parsing error: {str(e)}"}
        except Exception as e:
            logger.error(f"Unexpected error parsing LLM response: {e}")
            return {"action": "complete", "result": f"Task completed - Unexpected parsing error: {str(e)}"}

    async def _execute_tool_call(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        try:
            logger.info(f"Executing tool: {tool_name} with arguments: {arguments}")

            # éªŒè¯å·¥å…·æ˜¯å¦å­˜åœ¨
            tool_names = [tool["name"] for tool in self.available_tools]
            if tool_name not in tool_names:
                raise ValueError(f"Unknown tool: {tool_name}. Available tools: {tool_names}")

            # è°ƒç”¨MCPå·¥å…·
            result = await self.mcp_client.call_tool(tool_name, arguments)

            logger.info(f"Tool {tool_name} executed successfully")
            return result

        except Exception as e:
            logger.error(f"Error executing tool {tool_name}: {e}")
            return {"error": str(e)}

    async def __aenter__(self):
        await self.connect()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        try:
            await self.disconnect()
        except Exception as e:
            logger.warning(f"Error during context manager exit (this is often normal during shutdown): {e}")
        return False


# ä¾¿æ·å‡½æ•°
async def process_task(task_description: str, context: str = "") -> Dict[str, Any]:
    """
    ä¾¿æ·çš„ä»»åŠ¡å¤„ç†å‡½æ•°

    Args:
        task_description: ä»»åŠ¡æè¿°
        context: ä»»åŠ¡ä¸Šä¸‹æ–‡

    Returns:
        ä»»åŠ¡å¤„ç†ç»“æœ
    """
    async with LLMHost() as host:
        return await host.process_task(task_description, context)


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python llm_host.py <task_description> [context]")
        sys.exit(1)

    task_description = sys.argv[1]
    context = sys.argv[2] if len(sys.argv) > 2 else ""

    print(f"ğŸ¤– Starting intelligent task processing: {task_description}")
    if context:
        print(f"ğŸ“ Context: {context}")
    print("-" * 50)

    try:
        result = asyncio.run(process_task(task_description, context))
        print(f"\nâœ… Task completed!")
        print(f"Status: {result.get('status', 'unknown')}")
        print(f"Rounds used: {result.get('rounds_used', 0)}")
        print(f"Result: {result.get('result', 'No result')}")
    except Exception as e:
        print(f"\nâŒ Task failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
